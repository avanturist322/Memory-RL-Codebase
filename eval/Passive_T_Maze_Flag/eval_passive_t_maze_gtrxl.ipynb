{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "sys.path.append(\"../../models/episodic_transformer_memory_ppo\")\n",
    "\n",
    "from environments.Passive_T_Maze_Flag.env.env_passive_t_maze_flag import TMazeClassicPassive\n",
    "from models.episodic_transformer_memory_ppo.model import ActorCriticModel\n",
    "import os \n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import yaml\n",
    "import time\n",
    "from moviepy.editor import ImageSequenceClip, VideoFileClip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_transformer_memory(trxl_conf, max_episode_steps, device):\n",
    "    \"\"\"Returns initial tensors for the episodic memory of the transformer.\n",
    "\n",
    "    Arguments:\n",
    "        trxl_conf {dict} -- Transformer configuration dictionary\n",
    "        max_episode_steps {int} -- Maximum number of steps per episode\n",
    "        device {torch.device} -- Target device for the tensors\n",
    "\n",
    "    Returns:\n",
    "        memory {torch.Tensor}, memory_mask {torch.Tensor}, memory_indices {torch.Tensor} -- Initial episodic memory, episodic memory mask, and sliding memory window indices\n",
    "    \"\"\"\n",
    "    # Episodic memory mask used in attention\n",
    "    memory_mask = torch.tril(torch.ones((trxl_conf[\"memory_length\"], trxl_conf[\"memory_length\"])), diagonal=-1)\n",
    "    # Episdic memory tensor\n",
    "    memory = torch.zeros((1, max_episode_steps, trxl_conf[\"num_blocks\"], trxl_conf[\"embed_dim\"])).to(device)\n",
    "    # Setup sliding memory window indices\n",
    "    repetitions = torch.repeat_interleave(torch.arange(0, trxl_conf[\"memory_length\"]).unsqueeze(0), trxl_conf[\"memory_length\"] - 1, dim = 0).long()\n",
    "    memory_indices = torch.stack([torch.arange(i, i + trxl_conf[\"memory_length\"]) for i in range(max_episode_steps - trxl_conf[\"memory_length\"] + 1)]).long()\n",
    "    memory_indices = torch.cat((repetitions, memory_indices))\n",
    "    return memory, memory_mask, memory_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config_path = '/opt/Memory-RL-Codebase/configs/GTRXL_configs/MinigridMemory/Static/MinigridMemory_SHORT_TERM.yaml'\n",
    "config_path = '/opt/Memory-RL-Codebase/configs/GTRXL_configs/Passive_T_Maze_Flag/Dense/Passive_T_Maze_Flag_LONG_TERM.yaml'\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "\n",
    "with open(config_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "env = TMazeClassicPassive(episode_length=config['environment']['episode_timeout'], \n",
    "                            corridor_length=config['environment']['corridor_length'], \n",
    "                            goal_reward=config['environment']['goal_reward'],\n",
    "                            penalty=config['environment']['penalty'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckp 1\n",
    "\n",
    "checkpoint_path = '/opt/Memory-RL-Codebase/autorun/checkpoints_2024_09_29_19_00/Passive_T_Maze_Flag/GTXL/GTXL_Passive_T_Maze_Flag_LONG_TERM_dense/2024_09_29-12_25_10.pt'\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "config['transformer']['num_blocks'] = 3\n",
    "config['transformer']['embed_dim'] = 64\n",
    "config['transformer']['num_heads'] = 4\n",
    "config['hidden_layer_size'] = 64\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '/opt/Memory-RL-Codebase/autorun/checkpoints_2024_09_29_22_00/Passive_T_Maze_Flag/GTXL/GTXL_Passive_T_Maze_Flag_LONG_TERM_dense/2024_09_29-12_26_31.pt'\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "config['transformer']['num_blocks'] = 6\n",
    "config['transformer']['embed_dim'] = 128\n",
    "config['transformer']['num_heads'] = 8\n",
    "config['hidden_layer_size'] = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = '/opt/Memory-RL-Codebase/configs/GTRXL_configs/Passive_T_Maze_Flag/Dense/Passive_T_Maze_Flag_SHORT_TERM.yaml'\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "\n",
    "with open(config_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "env = TMazeClassicPassive(episode_length=config['environment']['episode_timeout'], \n",
    "                            corridor_length=config['environment']['corridor_length'], \n",
    "                            goal_reward=config['environment']['goal_reward'],\n",
    "                            penalty=config['environment']['penalty'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckp 1\n",
    "\n",
    "checkpoint_path = '/opt/Memory-RL-Codebase/autorun/checkpoints_2024_09_29_19_00/Passive_T_Maze_Flag/GTXL/GTXL_Passive_T_Maze_Flag_SHORT_TERM_dense/2024_09_29-12_25_14.pt'\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "config['transformer']['num_blocks'] = 3\n",
    "config['transformer']['embed_dim'] = 64\n",
    "config['transformer']['num_heads'] = 4\n",
    "config['hidden_layer_size'] = 64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckp 2\n",
    "\n",
    "checkpoint_path = '/opt/Memory-RL-Codebase/autorun/checkpoints_2024_09_29_22_00/Passive_T_Maze_Flag/GTXL/GTXL_Passive_T_Maze_Flag_SHORT_TERM_dense/2024_09_28-23_36_02.pt'\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "config['transformer']['num_blocks'] = 6\n",
    "config['transformer']['embed_dim'] = 128\n",
    "config['transformer']['num_heads'] = 8\n",
    "config['hidden_layer_size'] = 128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckp 3\n",
    "\n",
    "checkpoint_path = '/opt/Memory-RL-Codebase/autorun/checkpoints/Passive_T_Maze_Flag/GTXL/GTXL_Passive_T_Maze_Flag_SHORT_TERM_dense/2024_09_29-12_23_46.pt'\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "config['transformer']['num_blocks'] = 6\n",
    "config['transformer']['embed_dim'] = 128\n",
    "config['transformer']['num_heads'] = 8\n",
    "config['hidden_layer_size'] = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ActorCriticModel:\n\tsize mismatch for lin_hidden.weight: copying a param with shape torch.Size([256, 4]) from checkpoint, the shape in current model is torch.Size([128, 4]).\n\tsize mismatch for lin_hidden.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.linear_embedding.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.linear_embedding.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.pos_embedding.inv_freqs: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for transformer.transformer_blocks.0.attention.values.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.0.attention.keys.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.0.attention.queries.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.0.attention.fc_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.0.attention.fc_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.0.gate1.bg: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.0.gate1.Wr.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.0.gate1.Ur.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.0.gate1.Wz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.0.gate1.Uz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.0.gate1.Wg.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.0.gate1.Ug.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.0.gate2.bg: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.0.gate2.Wr.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.0.gate2.Ur.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.0.gate2.Wz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.0.gate2.Uz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.0.gate2.Wg.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.0.gate2.Ug.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.0.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.0.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.0.norm2.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.0.norm2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.0.norm_kv.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.0.norm_kv.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.0.fc.0.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.0.fc.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.1.attention.values.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.1.attention.keys.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.1.attention.queries.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.1.attention.fc_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.1.attention.fc_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.1.gate1.bg: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.1.gate1.Wr.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.1.gate1.Ur.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.1.gate1.Wz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.1.gate1.Uz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.1.gate1.Wg.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.1.gate1.Ug.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.1.gate2.bg: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.1.gate2.Wr.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.1.gate2.Ur.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.1.gate2.Wz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.1.gate2.Uz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.1.gate2.Wg.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.1.gate2.Ug.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.1.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.1.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.1.norm2.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.1.norm2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.1.norm_kv.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.1.norm_kv.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.1.fc.0.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.1.fc.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.2.attention.values.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.2.attention.keys.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.2.attention.queries.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.2.attention.fc_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.2.attention.fc_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.2.gate1.bg: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.2.gate1.Wr.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.2.gate1.Ur.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.2.gate1.Wz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.2.gate1.Uz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.2.gate1.Wg.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.2.gate1.Ug.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.2.gate2.bg: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.2.gate2.Wr.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.2.gate2.Ur.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.2.gate2.Wz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.2.gate2.Uz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.2.gate2.Wg.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.2.gate2.Ug.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.2.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.2.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.2.norm2.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.2.norm2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.2.norm_kv.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.2.norm_kv.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.2.fc.0.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.2.fc.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.3.attention.values.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.3.attention.keys.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.3.attention.queries.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.3.attention.fc_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.3.attention.fc_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.3.gate1.bg: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.3.gate1.Wr.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.3.gate1.Ur.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.3.gate1.Wz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.3.gate1.Uz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.3.gate1.Wg.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.3.gate1.Ug.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.3.gate2.bg: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.3.gate2.Wr.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.3.gate2.Ur.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.3.gate2.Wz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.3.gate2.Uz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.3.gate2.Wg.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.3.gate2.Ug.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.3.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.3.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.3.norm2.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.3.norm2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.3.norm_kv.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.3.norm_kv.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.3.fc.0.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.3.fc.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.4.attention.values.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.4.attention.keys.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.4.attention.queries.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.4.attention.fc_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.4.attention.fc_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.4.gate1.bg: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.4.gate1.Wr.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.4.gate1.Ur.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.4.gate1.Wz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.4.gate1.Uz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.4.gate1.Wg.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.4.gate1.Ug.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.4.gate2.bg: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.4.gate2.Wr.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.4.gate2.Ur.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.4.gate2.Wz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.4.gate2.Uz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.4.gate2.Wg.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.4.gate2.Ug.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.4.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.4.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.4.norm2.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.4.norm2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.4.norm_kv.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.4.norm_kv.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.4.fc.0.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.4.fc.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.5.attention.values.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.5.attention.keys.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.5.attention.queries.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.5.attention.fc_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.5.attention.fc_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.5.gate1.bg: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.5.gate1.Wr.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.5.gate1.Ur.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.5.gate1.Wz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.5.gate1.Uz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.5.gate1.Wg.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.5.gate1.Ug.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.5.gate2.bg: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.5.gate2.Wr.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.5.gate2.Ur.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.5.gate2.Wz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.5.gate2.Uz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.5.gate2.Wg.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.5.gate2.Ug.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.5.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.5.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.5.norm2.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.5.norm2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.5.norm_kv.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.5.norm_kv.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.5.fc.0.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.5.fc.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for lin_policy.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for lin_policy.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for lin_value.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for lin_value.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for policy_branches.0.weight: copying a param with shape torch.Size([4, 256]) from checkpoint, the shape in current model is torch.Size([4, 128]).\n\tsize mismatch for value.weight: copying a param with shape torch.Size([1, 256]) from checkpoint, the shape in current model is torch.Size([1, 128]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m agent \u001b[38;5;241m=\u001b[39m ActorCriticModel(config, env\u001b[38;5;241m.\u001b[39mobservation_space, (env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn,), env\u001b[38;5;241m.\u001b[39mmax_episode_steps)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 2\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m agent\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      4\u001b[0m agent \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ActorCriticModel:\n\tsize mismatch for lin_hidden.weight: copying a param with shape torch.Size([256, 4]) from checkpoint, the shape in current model is torch.Size([128, 4]).\n\tsize mismatch for lin_hidden.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.linear_embedding.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.linear_embedding.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.pos_embedding.inv_freqs: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for transformer.transformer_blocks.0.attention.values.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.0.attention.keys.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.0.attention.queries.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.0.attention.fc_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.0.attention.fc_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.0.gate1.bg: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.0.gate1.Wr.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.0.gate1.Ur.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.0.gate1.Wz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.0.gate1.Uz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.0.gate1.Wg.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.0.gate1.Ug.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.0.gate2.bg: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.0.gate2.Wr.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.0.gate2.Ur.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.0.gate2.Wz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.0.gate2.Uz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.0.gate2.Wg.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.0.gate2.Ug.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.0.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.0.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.0.norm2.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.0.norm2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.0.norm_kv.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.0.norm_kv.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.0.fc.0.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.0.fc.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.1.attention.values.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.1.attention.keys.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.1.attention.queries.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.1.attention.fc_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.1.attention.fc_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.1.gate1.bg: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.1.gate1.Wr.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.1.gate1.Ur.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.1.gate1.Wz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.1.gate1.Uz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.1.gate1.Wg.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.1.gate1.Ug.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.1.gate2.bg: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.1.gate2.Wr.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.1.gate2.Ur.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.1.gate2.Wz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.1.gate2.Uz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.1.gate2.Wg.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.1.gate2.Ug.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.1.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.1.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.1.norm2.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.1.norm2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.1.norm_kv.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.1.norm_kv.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.1.fc.0.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.1.fc.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.2.attention.values.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.2.attention.keys.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.2.attention.queries.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.2.attention.fc_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.2.attention.fc_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.2.gate1.bg: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.2.gate1.Wr.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.2.gate1.Ur.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.2.gate1.Wz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.2.gate1.Uz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.2.gate1.Wg.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.2.gate1.Ug.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.2.gate2.bg: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.2.gate2.Wr.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.2.gate2.Ur.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.2.gate2.Wz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.2.gate2.Uz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.2.gate2.Wg.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.2.gate2.Ug.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.2.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.2.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.2.norm2.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.2.norm2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.2.norm_kv.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.2.norm_kv.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.2.fc.0.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.2.fc.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.3.attention.values.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.3.attention.keys.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.3.attention.queries.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.3.attention.fc_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.3.attention.fc_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.3.gate1.bg: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.3.gate1.Wr.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.3.gate1.Ur.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.3.gate1.Wz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.3.gate1.Uz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.3.gate1.Wg.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.3.gate1.Ug.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.3.gate2.bg: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.3.gate2.Wr.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.3.gate2.Ur.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.3.gate2.Wz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.3.gate2.Uz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.3.gate2.Wg.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.3.gate2.Ug.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.3.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.3.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.3.norm2.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.3.norm2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.3.norm_kv.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.3.norm_kv.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.3.fc.0.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.3.fc.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.4.attention.values.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.4.attention.keys.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.4.attention.queries.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.4.attention.fc_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.4.attention.fc_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.4.gate1.bg: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.4.gate1.Wr.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.4.gate1.Ur.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.4.gate1.Wz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.4.gate1.Uz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.4.gate1.Wg.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.4.gate1.Ug.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.4.gate2.bg: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.4.gate2.Wr.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.4.gate2.Ur.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.4.gate2.Wz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.4.gate2.Uz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.4.gate2.Wg.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.4.gate2.Ug.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.4.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.4.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.4.norm2.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.4.norm2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.4.norm_kv.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.4.norm_kv.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.4.fc.0.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.4.fc.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.5.attention.values.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.5.attention.keys.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.5.attention.queries.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.5.attention.fc_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.5.attention.fc_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.5.gate1.bg: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.5.gate1.Wr.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.5.gate1.Ur.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.5.gate1.Wz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.5.gate1.Uz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.5.gate1.Wg.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.5.gate1.Ug.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.5.gate2.bg: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.5.gate2.Wr.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.5.gate2.Ur.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.5.gate2.Wz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.5.gate2.Uz.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.5.gate2.Wg.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.5.gate2.Ug.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.5.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.5.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.5.norm2.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.5.norm2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.5.norm_kv.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.5.norm_kv.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.transformer_blocks.5.fc.0.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.transformer_blocks.5.fc.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for lin_policy.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for lin_policy.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for lin_value.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for lin_value.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for policy_branches.0.weight: copying a param with shape torch.Size([4, 256]) from checkpoint, the shape in current model is torch.Size([4, 128]).\n\tsize mismatch for value.weight: copying a param with shape torch.Size([1, 256]) from checkpoint, the shape in current model is torch.Size([1, 128])."
     ]
    }
   ],
   "source": [
    "agent = ActorCriticModel(config, env.observation_space, (env.action_space.n,), env.max_episode_steps).to(device)\n",
    "agent.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "agent.eval()\n",
    "agent = agent.to(device)\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "def generate_permutations(nums):\n",
    "\n",
    "    perms = permutations(nums)\n",
    "    result = [int(''.join(map(str, perm))) for perm in perms]\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### evaluate !\n",
    "\n",
    "videos_dir = '/opt/Memory-RL-Codebase/eval/Minigrid_Memory/GTRXL'\n",
    "\n",
    "nums = [1, 2, 3, 4, 5]\n",
    "eval_seeds = generate_permutations(nums)\n",
    "\n",
    "videos_limit = len(eval_seeds) + 1\n",
    "n_episode = len(eval_seeds)\n",
    "\n",
    "\n",
    "render = False\n",
    "\n",
    "total_reward = 0\n",
    "num_successes = 0\n",
    "total_steps = 0\n",
    "\n",
    "\n",
    "for i in range(n_episode):\n",
    "\n",
    "    if render:\n",
    "        frames = []\n",
    "\n",
    "    done = False\n",
    "    memory, memory_mask, memory_indices = init_transformer_memory(config[\"transformer\"], env.max_episode_steps, device)\n",
    "\n",
    "    memory = memory.to(device)\n",
    "    memory_mask = memory_mask.to(device)\n",
    "    memory_indices = memory_indices.to(device)\n",
    "\n",
    "\n",
    "    memory_length = config[\"transformer\"][\"memory_length\"]\n",
    "    # eval_seeds = config.get(\"eval_seeds\", None)\n",
    "    t = 0\n",
    "    ep_reward = 0\n",
    "\n",
    "    if eval_seeds is not None:\n",
    "        obs = env.reset(eval_seeds[i])    \n",
    "    else:\n",
    "        obs = env.reset()\n",
    "\n",
    "    if render and i < videos_limit:\n",
    "        rofl = env.render()\n",
    "        time.sleep(0.5)\n",
    "        frames.append(rofl)\n",
    "\n",
    "\n",
    "\n",
    "    while not done:\n",
    "        # Prepare observation and memory\n",
    "        obs = torch.tensor(np.expand_dims(obs, 0), dtype=torch.float32, device=device)\n",
    "        in_memory = memory[0, memory_indices[t].unsqueeze(0)]\n",
    "        t_ = max(0, min(t, memory_length - 1))\n",
    "        mask = memory_mask[t_].unsqueeze(0)\n",
    "        indices = memory_indices[t].unsqueeze(0)\n",
    "        # Forward model\n",
    "        policy, value, new_memory = agent(obs.to(device), in_memory.to(device), mask.to(device), indices.to(device))\n",
    "        memory[:, t] = new_memory\n",
    "        # Sample action\n",
    "        action = []\n",
    "        for action_branch in policy:\n",
    "            action.append(action_branch.sample().item())\n",
    "        # Step environemnt\n",
    "        # print(f'action: {action}')\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        # print(f'Action :{action}, obs: {obs.shape}, reward: {reward}, terminated: {done}, info: {info}')\n",
    "        if render and i < videos_limit:\n",
    "            rofl = env.render()\n",
    "            if done:\n",
    "                print(f\"Episode terminated. Episode reward: {ep_reward}\")\n",
    "            time.sleep(0.5)\n",
    "            frames.append(rofl)\n",
    "\n",
    "\n",
    "\n",
    "        ep_reward += reward\n",
    "        t += 1\n",
    "\n",
    "\n",
    "    if info.get(\"is_success\"):\n",
    "        num_successes += 1\n",
    "    total_reward += ep_reward\n",
    "    total_steps += t\n",
    "\n",
    "    if render and i < videos_limit:\n",
    "        desired_resolution = (945, 540)\n",
    "        original_aspect_ratio = 112 / 64\n",
    "        width = int(desired_resolution[0] * original_aspect_ratio)\n",
    "        height = desired_resolution[1]\n",
    "\n",
    "        observations = [np.squeeze(o) for o in frames]\n",
    "\n",
    "        clip = ImageSequenceClip(observations, fps=2)\n",
    "        clip = clip.resize(width=width, height=height)\n",
    "\n",
    "\n",
    "        run_name = checkpoint_path.split('/')[-1].strip('.pt')\n",
    "        run_type = checkpoint_path.split('/')[-2]\n",
    "        curr_seed = eval_seeds[i]\n",
    "        curr_reward = float(info['reward'])\n",
    "\n",
    "        if not os.path.exists(videos_dir + f\"/{run_type}/{run_name}\"):\n",
    "            os.makedirs(videos_dir + f\"/{run_type}/{run_name}\")\n",
    "\n",
    "        clip.write_videofile(videos_dir + f\"/{run_type}/{run_name}/{run_name}_seed={curr_seed}_reward={curr_reward:0.2}.mp4\", fps=2)\n",
    "\n",
    "    curr_seed = eval_seeds[i]\n",
    "    print(f'Episode: {i}, seed: {curr_seed} Reward: {ep_reward}, Steps: {t} Mean reward: {total_reward / (i + 1)}, Mean steps: {total_steps / (i + 1)}')\n",
    "\n",
    "\n",
    "print(f'Total num episodes: {n_episode} Success rate: {num_successes / n_episode}, Mean reward: {total_reward / n_episode}, Mean steps: {total_steps / n_episode}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
