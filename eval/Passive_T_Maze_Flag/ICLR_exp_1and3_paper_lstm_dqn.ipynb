{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "sys.path.append(\"../../models/Memory_RL\")\n",
    "\n",
    "# from environments.Passive_T_Maze_Flag.env.env_passive_t_maze_flag import TMazeClassicPassive\n",
    "from models.Memory_RL.envs.tmaze import TMazeClassicPassive\n",
    "from models.Memory_RL.policies.models.policy_rnn_dqn import ModelFreeOffPolicy_DQN_RNN\n",
    "import os \n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import yaml\n",
    "import time\n",
    "from moviepy.editor import ImageSequenceClip, VideoFileClip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.rl.name_fns import name_fn as name_fn1\n",
    "from ml_collections import ConfigDict\n",
    "from typing import Tuple\n",
    "from torchkit import pytorch_utils as ptu\n",
    "\n",
    "def dqn_name_fn(\n",
    "    config: ConfigDict, max_episode_steps: int, max_training_steps: int\n",
    ") -> Tuple[ConfigDict, str]:\n",
    "    config, name = name_fn1(config)\n",
    "    # set eps = 1/T, so that the asymptotic prob to\n",
    "    # sample fully exploited trajectory during exploration is\n",
    "    # (1-1/T)^T = 1/e\n",
    "    config.init_eps = 1.0\n",
    "    config.end_eps = 1.0 / max_episode_steps\n",
    "    config.schedule_steps = config.schedule_end * max_training_steps\n",
    "\n",
    "    return config, name\n",
    "\n",
    "\n",
    "def get_rl_config():\n",
    "    config = ConfigDict()\n",
    "    config.name_fn = dqn_name_fn\n",
    "\n",
    "    config.algo = \"dqn\"\n",
    "\n",
    "    config.critic_lr = 3e-4\n",
    "\n",
    "    config.config_critic = ConfigDict()\n",
    "    config.config_critic.hidden_dims = (256, 256)\n",
    "\n",
    "    config.discount = 0.99\n",
    "    config.tau = 0.005\n",
    "    config.schedule_end = 0.1  # at least good for TMaze-like envs\n",
    "\n",
    "    config.replay_buffer_size = 1e6\n",
    "    config.replay_buffer_num_episodes = 1e3\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_collections import ConfigDict\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def name_fn(config: ConfigDict, max_episode_steps: int) -> Tuple[ConfigDict, str]:\n",
    "    name = \"\"\n",
    "\n",
    "    if config.sampled_seq_len == -1:\n",
    "        config.sampled_seq_len = max_episode_steps\n",
    "\n",
    "    name += f\"{config.model.seq_model_config.name}-len-{config.sampled_seq_len}/\"\n",
    "\n",
    "    assert config.clip is False\n",
    "\n",
    "    del config.name_fn\n",
    "    return config, name\n",
    "\n",
    "\n",
    "def get_seq_config():\n",
    "    config = ConfigDict()\n",
    "    config.name_fn = name_fn\n",
    "\n",
    "    config.is_markov = False\n",
    "    config.is_attn = False\n",
    "    config.use_dropout = False\n",
    "\n",
    "    config.sampled_seq_len = -1\n",
    "\n",
    "    config.clip = False\n",
    "    config.max_norm = 1.0\n",
    "    config.use_l2_norm = False\n",
    "\n",
    "    # fed into Module\n",
    "    config.model = ConfigDict()\n",
    "\n",
    "    # seq_model specific\n",
    "    config.model.seq_model_config = ConfigDict()\n",
    "    config.model.seq_model_config.name = \"lstm\"\n",
    "    config.model.seq_model_config.hidden_size = 128\n",
    "    config.model.seq_model_config.n_layer = 1\n",
    "\n",
    "    # embedders\n",
    "    config.model.observ_embedder = ConfigDict()\n",
    "    config.model.observ_embedder.name = \"mlp\"\n",
    "    config.model.observ_embedder.hidden_size = 32\n",
    "\n",
    "    config.model.action_embedder = ConfigDict()\n",
    "    config.model.action_embedder.name = \"mlp\"\n",
    "    config.model.action_embedder.hidden_size = 16\n",
    "\n",
    "    config.model.reward_embedder = ConfigDict()\n",
    "    config.model.reward_embedder.name = \"mlp\"\n",
    "    config.model.reward_embedder.hidden_size = 0\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "def generate_permutations(nums):\n",
    "\n",
    "    perms = permutations(nums)\n",
    "    result = [int(''.join(map(str, perm))) for perm in perms]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# AGENT_CLASSES = {\n",
    "#     \"Policy_MLP\": Policy_MLP,\n",
    "#     \"Policy_RNN_MLP\": Policy_RNN_MLP,\n",
    "#     \"Policy_Separate_RNN\": Policy_Separate_RNN,\n",
    "#     \"Policy_Shared_RNN\": Policy_Shared_RNN,\n",
    "#     \"Policy_DQN_RNN\": Policy_DQN_RNN,\n",
    "# }\n",
    "from torchkit.pytorch_utils import set_gpu_mode\n",
    "set_gpu_mode('cuda', 0)\n",
    "\n",
    "agent_class = ModelFreeOffPolicy_DQN_RNN\n",
    "agent_arch = agent_class.ARCH\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "episode_timeout = 31\n",
    "corridor_length = episode_timeout - 2\n",
    "penalty = -1/(episode_timeout - 1)\n",
    "\n",
    "\n",
    "env = TMazeClassicPassive(episode_length=episode_timeout, \n",
    "                            corridor_length=corridor_length, \n",
    "                            goal_reward=1.0,\n",
    "                            penalty=penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episode_steps = 15\n",
    "max_training_steps = 999\n",
    "\n",
    "config_seq, _ = name_fn(get_seq_config(), max_episode_steps = max_episode_steps)\n",
    "config_rl, _ = dqn_name_fn(config = get_rl_config(), max_episode_steps =max_episode_steps , max_training_steps =max_training_steps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder_fn = lambda: None\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = 4\n",
    "\n",
    "freeze_critic = False\n",
    "\n",
    "agent = agent_class(\n",
    "    obs_dim=obs_dim,\n",
    "    action_dim=act_dim,\n",
    "    config_seq=config_seq,\n",
    "    config_rl=config_rl,\n",
    "    image_encoder_fn=image_encoder_fn,\n",
    "    freeze_critic=freeze_critic,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = '/opt/Memory-RL-Codebase/models/Memory_RL/logs_2024_09_30_23_00/MinigridMemory/LSTM_DQN/SHORT_TERM/2024_09_30-17_33_03/best_agent.pt'\n",
    "# ckpt_path = '/opt/Memory-RL-Codebase/models/Memory_RL/logs_2024_09_30_12_00/LSTM_DQN/2024_09_30-02_28_51/curr_agent.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import helpers as utl\n",
    "\n",
    "deterministic = False\n",
    "# eval_episodes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = agent.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = [1, 2, 3, 4, 5]\n",
    "eval_seeds = generate_permutations(nums)\n",
    "\n",
    "videos_limit = len(eval_seeds) + 1\n",
    "n_episode = len(eval_seeds)\n",
    "\n",
    "\n",
    "render = False\n",
    "\n",
    "total_reward = 0\n",
    "num_successes = 0\n",
    "total_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, seed: 12345 Reward: -0.9999999962747097, Steps: 30 Mean reward: -0.9999999962747097, Mean steps: 30.0\n",
      "Episode: 1, seed: 12354 Reward: -0.931034479290247, Steps: 30 Mean reward: -0.9655172377824783, Mean steps: 30.0\n",
      "Episode: 2, seed: 12435 Reward: -0.8965517207980156, Steps: 30 Mean reward: -0.9425287321209908, Mean steps: 30.0\n",
      "Episode: 3, seed: 12453 Reward: -0.931034479290247, Steps: 30 Mean reward: -0.9396551689133048, Mean steps: 30.0\n",
      "Episode: 4, seed: 12534 Reward: -0.6896551698446274, Steps: 30 Mean reward: -0.8896551690995693, Mean steps: 30.0\n",
      "Episode: 5, seed: 12543 Reward: -0.41379310190677643, Steps: 30 Mean reward: -0.8103448245674372, Mean steps: 30.0\n",
      "Episode: 6, seed: 13245 Reward: -0.6896551698446274, Steps: 30 Mean reward: -0.7931034453213215, Mean steps: 30.0\n",
      "Episode: 7, seed: 13254 Reward: -0.931034479290247, Steps: 30 Mean reward: -0.8103448245674372, Mean steps: 30.0\n",
      "Episode: 8, seed: 13425 Reward: -0.7586206868290901, Steps: 30 Mean reward: -0.8045976981520653, Mean steps: 30.0\n",
      "Episode: 9, seed: 13452 Reward: -0.48275861889123917, Steps: 30 Mean reward: -0.7724137902259827, Mean steps: 30.0\n",
      "Episode: 10, seed: 13524 Reward: -0.48275861889123917, Steps: 30 Mean reward: -0.7460815019228242, Mean steps: 30.0\n",
      "Episode: 11, seed: 13542 Reward: -0.48275861889123917, Steps: 30 Mean reward: -0.7241379283368587, Mean steps: 30.0\n",
      "Episode: 12, seed: 14235 Reward: -0.4482758603990078, Steps: 30 Mean reward: -0.7029177692647164, Mean steps: 30.0\n",
      "Episode: 13, seed: 14253 Reward: -0.4482758603990078, Steps: 30 Mean reward: -0.6847290614885944, Mean steps: 30.0\n",
      "Episode: 14, seed: 14325 Reward: -0.655172411352396, Steps: 30 Mean reward: -0.6827586181461811, Mean steps: 30.0\n",
      "Episode: 15, seed: 14352 Reward: -0.7241379283368587, Steps: 30 Mean reward: -0.6853448250330985, Mean steps: 30.0\n",
      "Episode: 16, seed: 14523 Reward: -0.6206896528601646, Steps: 30 Mean reward: -0.6815415796111611, Mean steps: 30.0\n",
      "Episode: 17, seed: 14532 Reward: -0.41379310190677643, Steps: 30 Mean reward: -0.6666666641831398, Mean steps: 30.0\n",
      "Episode: 18, seed: 15234 Reward: -0.7931034453213215, Steps: 30 Mean reward: -0.6733212316114652, Mean steps: 30.0\n",
      "Episode: 19, seed: 15243 Reward: -0.41379310190677643, Steps: 30 Mean reward: -0.6603448251262307, Mean steps: 30.0\n",
      "Episode: 20, seed: 15324 Reward: -0.8275862038135529, Steps: 30 Mean reward: -0.6683087003018174, Mean steps: 30.0\n",
      "Episode: 21, seed: 15342 Reward: -0.655172411352396, Steps: 30 Mean reward: -0.6677115962586619, Mean steps: 30.0\n",
      "Episode: 22, seed: 15423 Reward: -0.4482758603990078, Steps: 30 Mean reward: -0.6581709120908509, Mean steps: 30.0\n",
      "Episode: 23, seed: 15432 Reward: -0.8275862038135529, Steps: 30 Mean reward: -0.6652298825792968, Mean steps: 30.0\n",
      "Episode: 24, seed: 21345 Reward: -0.41379310190677643, Steps: 30 Mean reward: -0.655172411352396, Mean steps: 30.0\n",
      "Episode: 25, seed: 21354 Reward: -0.9655172377824783, Steps: 30 Mean reward: -0.667108750830476, Mean steps: 30.0\n",
      "Episode: 26, seed: 21435 Reward: -0.4482758603990078, Steps: 30 Mean reward: -0.659003828962644, Mean steps: 30.0\n",
      "Episode: 27, seed: 21453 Reward: -0.7586206868290901, Steps: 30 Mean reward: -0.6625615738864455, Mean steps: 30.0\n",
      "Episode: 28, seed: 21534 Reward: -0.9655172377824783, Steps: 30 Mean reward: -0.6730083209173433, Mean steps: 30.0\n",
      "Episode: 29, seed: 21543 Reward: -0.48275861889123917, Steps: 30 Mean reward: -0.6666666641831398, Mean steps: 30.0\n",
      "Episode: 30, seed: 23145 Reward: -0.4482758603990078, Steps: 30 Mean reward: -0.659621799544942, Mean steps: 30.0\n",
      "Episode: 31, seed: 23154 Reward: -0.4482758603990078, Steps: 30 Mean reward: -0.6530172389466316, Mean steps: 30.0\n",
      "Episode: 32, seed: 23415 Reward: -0.41379310190677643, Steps: 30 Mean reward: -0.6457680226726965, Mean steps: 30.0\n",
      "Episode: 33, seed: 23451 Reward: -0.9655172377824783, Steps: 30 Mean reward: -0.655172411352396, Mean steps: 30.0\n",
      "Episode: 34, seed: 23514 Reward: -0.7241379283368587, Steps: 30 Mean reward: -0.6571428546948093, Mean steps: 30.0\n",
      "Episode: 35, seed: 23541 Reward: -0.7241379283368587, Steps: 30 Mean reward: -0.659003828962644, Mean steps: 30.0\n",
      "Episode: 36, seed: 24135 Reward: -0.41379310190677643, Steps: 30 Mean reward: -0.6523765120151881, Mean steps: 30.0\n",
      "Episode: 37, seed: 24153 Reward: -0.5862068943679333, Steps: 30 Mean reward: -0.6506352062876287, Mean steps: 30.0\n",
      "Episode: 38, seed: 24315 Reward: -0.4482758603990078, Steps: 30 Mean reward: -0.6454465051109974, Mean steps: 30.0\n",
      "Episode: 39, seed: 24351 Reward: -0.4482758603990078, Steps: 30 Mean reward: -0.6405172389931977, Mean steps: 30.0\n",
      "Episode: 40, seed: 24513 Reward: -0.48275861889123917, Steps: 30 Mean reward: -0.6366694677711987, Mean steps: 30.0\n",
      "Episode: 41, seed: 24531 Reward: -0.4482758603990078, Steps: 30 Mean reward: -0.6321839056909084, Mean steps: 30.0\n",
      "Episode: 42, seed: 25134 Reward: -0.5862068943679333, Steps: 30 Mean reward: -0.6311146728694439, Mean steps: 30.0\n",
      "Episode: 43, seed: 25143 Reward: -0.4482758603990078, Steps: 30 Mean reward: -0.6269592453132976, Mean steps: 30.0\n",
      "Episode: 44, seed: 25314 Reward: -0.8620689623057842, Steps: 30 Mean reward: -0.6321839056909084, Mean steps: 30.0\n",
      "Episode: 45, seed: 25341 Reward: -0.6206896528601646, Steps: 30 Mean reward: -0.6319340306293705, Mean steps: 30.0\n",
      "Episode: 46, seed: 25413 Reward: -0.8620689623057842, Steps: 30 Mean reward: -0.6368305185373794, Mean steps: 30.0\n",
      "Episode: 47, seed: 25431 Reward: -0.9655172377824783, Steps: 30 Mean reward: -0.6436781585216522, Mean steps: 30.0\n",
      "Episode: 48, seed: 31245 Reward: -0.655172411352396, Steps: 30 Mean reward: -0.6439127351100348, Mean steps: 30.0\n",
      "Episode: 49, seed: 31254 Reward: -0.6896551698446274, Steps: 30 Mean reward: -0.6448275838047266, Mean steps: 30.0\n",
      "Episode: 50, seed: 31425 Reward: -0.4482758603990078, Steps: 30 Mean reward: -0.6409736284438301, Mean steps: 30.0\n",
      "Episode: 51, seed: 31452 Reward: -0.4482758603990078, Steps: 30 Mean reward: -0.6372679021352758, Mean steps: 30.0\n",
      "Episode: 52, seed: 31524 Reward: -0.4482758603990078, Steps: 30 Mean reward: -0.6337020145553462, Mean steps: 30.0\n",
      "Episode: 53, seed: 31542 Reward: -0.4482758603990078, Steps: 30 Mean reward: -0.6302681968857845, Mean steps: 30.0\n",
      "Episode: 54, seed: 32145 Reward: -0.41379310190677643, Steps: 30 Mean reward: -0.6263322860679843, Mean steps: 30.0\n",
      "Episode: 55, seed: 32154 Reward: -0.41379310190677643, Steps: 30 Mean reward: -0.622536943493677, Mean steps: 30.0\n",
      "Episode: 56, seed: 32415 Reward: -0.6896551698446274, Steps: 30 Mean reward: -0.6237144562366762, Mean steps: 30.0\n",
      "Episode: 57, seed: 32451 Reward: -0.5517241358757019, Steps: 30 Mean reward: -0.6224732438166594, Mean steps: 30.0\n",
      "Episode: 58, seed: 32514 Reward: -0.4482758603990078, Steps: 30 Mean reward: -0.6195207457926314, Mean steps: 30.0\n",
      "Episode: 59, seed: 32541 Reward: -0.6206896528601646, Steps: 30 Mean reward: -0.6195402275770903, Mean steps: 30.0\n",
      "Episode: 60, seed: 34125 Reward: -0.8620689623057842, Steps: 30 Mean reward: -0.623516108474282, Mean steps: 30.0\n",
      "Episode: 61, seed: 34152 Reward: -0.655172411352396, Steps: 30 Mean reward: -0.6240266940045741, Mean steps: 30.0\n",
      "Episode: 62, seed: 34215 Reward: -0.41379310190677643, Steps: 30 Mean reward: -0.6206896528601646, Mean steps: 30.0\n",
      "Episode: 63, seed: 34251 Reward: -0.655172411352396, Steps: 30 Mean reward: -0.6212284459616058, Mean steps: 30.0\n",
      "Episode: 64, seed: 34512 Reward: -0.4482758603990078, Steps: 30 Mean reward: -0.6185676369529504, Mean steps: 30.0\n",
      "Episode: 65, seed: 34521 Reward: -0.4482758603990078, Steps: 30 Mean reward: -0.6159874585203149, Mean steps: 30.0\n",
      "Episode: 66, seed: 35124 Reward: -0.7241379283368587, Steps: 30 Mean reward: -0.6176016446369798, Mean steps: 30.0\n",
      "Episode: 67, seed: 35142 Reward: -0.7586206868290901, Steps: 30 Mean reward: -0.6196754540809813, Mean steps: 30.0\n",
      "Episode: 68, seed: 35214 Reward: -0.4482758603990078, Steps: 30 Mean reward: -0.6171914019986339, Mean steps: 30.0\n",
      "Episode: 69, seed: 35241 Reward: -0.4482758603990078, Steps: 30 Mean reward: -0.614778322832925, Mean steps: 30.0\n",
      "Episode: 70, seed: 35412 Reward: -0.9999999962747097, Steps: 30 Mean reward: -0.6202039802053445, Mean steps: 30.0\n",
      "Episode: 71, seed: 35421 Reward: -0.4482758603990078, Steps: 30 Mean reward: -0.6178160896524787, Mean steps: 30.0\n",
      "Episode: 72, seed: 41235 Reward: -0.41379310190677643, Steps: 30 Mean reward: -0.6150212542039074, Mean steps: 30.0\n",
      "Episode: 73, seed: 41253 Reward: -0.7931034453213215, Steps: 30 Mean reward: -0.6174277703000887, Mean steps: 30.0\n",
      "Episode: 74, seed: 41325 Reward: -0.4482758603990078, Steps: 30 Mean reward: -0.6151724115014077, Mean steps: 30.0\n",
      "Episode: 75, seed: 41352 Reward: -0.7586206868290901, Steps: 30 Mean reward: -0.6170598888083508, Mean steps: 30.0\n",
      "Episode: 76, seed: 41523 Reward: -0.931034479290247, Steps: 30 Mean reward: -0.6211374808925313, Mean steps: 30.0\n",
      "Episode: 77, seed: 41532 Reward: -0.41379310190677643, Steps: 30 Mean reward: -0.6184792196234832, Mean steps: 30.0\n",
      "Episode: 78, seed: 42135 Reward: -0.5517241358757019, Steps: 30 Mean reward: -0.6176342185633846, Mean steps: 30.0\n",
      "Episode: 79, seed: 42153 Reward: -0.8965517207980156, Steps: 30 Mean reward: -0.6211206873413175, Mean steps: 30.0\n",
      "Episode: 80, seed: 42315 Reward: -0.9655172377824783, Steps: 30 Mean reward: -0.6253724966060232, Mean steps: 30.0\n",
      "Episode: 81, seed: 42351 Reward: -0.6206896528601646, Steps: 30 Mean reward: -0.625315388755464, Mean steps: 30.0\n",
      "Episode: 82, seed: 42513 Reward: -0.4482758603990078, Steps: 30 Mean reward: -0.6231823823897236, Mean steps: 30.0\n",
      "Episode: 83, seed: 42531 Reward: -0.655172411352396, Steps: 30 Mean reward: -0.6235632160678506, Mean steps: 30.0\n",
      "Episode: 84, seed: 43125 Reward: -0.4482758603990078, Steps: 30 Mean reward: -0.6215010118835113, Mean steps: 30.0\n",
      "Episode: 85, seed: 43152 Reward: -0.6896551698446274, Steps: 30 Mean reward: -0.6222935020923615, Mean steps: 30.0\n",
      "Episode: 86, seed: 43215 Reward: -0.48275861889123917, Steps: 30 Mean reward: -0.6206896528601646, Mean steps: 30.0\n",
      "Episode: 87, seed: 43251 Reward: -0.7586206868290901, Steps: 30 Mean reward: -0.6222570509734479, Mean steps: 30.0\n",
      "Episode: 88, seed: 43512 Reward: -0.6206896528601646, Steps: 30 Mean reward: -0.6222394397586919, Mean steps: 30.0\n",
      "Episode: 89, seed: 43521 Reward: -0.6896551698446274, Steps: 30 Mean reward: -0.6229885034263134, Mean steps: 30.0\n",
      "Episode: 90, seed: 45123 Reward: -0.41379310190677643, Steps: 30 Mean reward: -0.6206896528601646, Mean steps: 30.0\n",
      "Episode: 91, seed: 45132 Reward: -0.41379310190677643, Steps: 30 Mean reward: -0.6184407773063235, Mean steps: 30.0\n",
      "Episode: 92, seed: 45213 Reward: -0.6206896528601646, Steps: 30 Mean reward: -0.6184649587638916, Mean steps: 30.0\n",
      "Episode: 93, seed: 45231 Reward: -0.4482758603990078, Steps: 30 Mean reward: -0.6166544364408609, Mean steps: 30.0\n",
      "Episode: 94, seed: 45312 Reward: -0.9999999962747097, Steps: 30 Mean reward: -0.6206896528601646, Mean steps: 30.0\n",
      "Episode: 95, seed: 45321 Reward: -0.41379310190677643, Steps: 30 Mean reward: -0.6185344804544002, Mean steps: 30.0\n",
      "Episode: 96, seed: 51234 Reward: -0.4482758603990078, Steps: 30 Mean reward: -0.6167792369486745, Mean steps: 30.0\n",
      "Episode: 97, seed: 51243 Reward: -0.41379310190677643, Steps: 30 Mean reward: -0.6147079498564102, Mean steps: 30.0\n",
      "Episode: 98, seed: 51324 Reward: -0.9999999962747097, Steps: 30 Mean reward: -0.6185997887091204, Mean steps: 30.0\n",
      "Episode: 99, seed: 51342 Reward: -0.5517241358757019, Steps: 30 Mean reward: -0.6179310321807862, Mean steps: 30.0\n",
      "Episode: 100, seed: 51423 Reward: -0.8620689623057842, Steps: 30 Mean reward: -0.6203482394097465, Mean steps: 30.0\n",
      "Episode: 101, seed: 51432 Reward: -0.4482758603990078, Steps: 30 Mean reward: -0.618661255301798, Mean steps: 30.0\n",
      "Episode: 102, seed: 52134 Reward: -0.9655172377824783, Steps: 30 Mean reward: -0.6220287891122901, Mean steps: 30.0\n",
      "Episode: 103, seed: 52143 Reward: -0.41379310190677643, Steps: 30 Mean reward: -0.6200265228891602, Mean steps: 30.0\n",
      "Episode: 104, seed: 52314 Reward: -0.7931034453213215, Steps: 30 Mean reward: -0.6216748745313713, Mean steps: 30.0\n",
      "Episode: 105, seed: 52341 Reward: -0.931034479290247, Steps: 30 Mean reward: -0.6245933613687191, Mean steps: 30.0\n",
      "Episode: 106, seed: 52413 Reward: -0.931034479290247, Steps: 30 Mean reward: -0.6274572970502288, Mean steps: 30.0\n",
      "Episode: 107, seed: 52431 Reward: -0.6206896528601646, Steps: 30 Mean reward: -0.6273946336780986, Mean steps: 30.0\n",
      "Episode: 108, seed: 53124 Reward: -0.41379310190677643, Steps: 30 Mean reward: -0.6254349865976276, Mean steps: 30.0\n",
      "Episode: 109, seed: 53142 Reward: -0.4482758603990078, Steps: 30 Mean reward: -0.6238244490867312, Mean steps: 30.0\n",
      "Episode: 110, seed: 53214 Reward: -0.4482758603990078, Steps: 30 Mean reward: -0.6222429302697247, Mean steps: 30.0\n",
      "Episode: 111, seed: 53241 Reward: -0.931034479290247, Steps: 30 Mean reward: -0.6249999976716936, Mean steps: 30.0\n",
      "Episode: 112, seed: 53412 Reward: -0.655172411352396, Steps: 30 Mean reward: -0.6252670101821423, Mean steps: 30.0\n",
      "Episode: 113, seed: 53421 Reward: -0.41379310190677643, Steps: 30 Mean reward: -0.623411975899025, Mean steps: 30.0\n",
      "Episode: 114, seed: 54123 Reward: -0.4482758603990078, Steps: 30 Mean reward: -0.6218890531555467, Mean steps: 30.0\n",
      "Episode: 115, seed: 54132 Reward: -0.4482758603990078, Steps: 30 Mean reward: -0.6203923877007489, Mean steps: 30.0\n",
      "Episode: 116, seed: 54213 Reward: -0.4482758603990078, Steps: 30 Mean reward: -0.6189213062708194, Mean steps: 30.0\n",
      "Episode: 117, seed: 54231 Reward: -0.41379310190677643, Steps: 30 Mean reward: -0.6171829316575649, Mean steps: 30.0\n",
      "Episode: 118, seed: 54312 Reward: -0.7241379283368587, Steps: 30 Mean reward: -0.6180817131422648, Mean steps: 30.0\n",
      "Episode: 119, seed: 54321 Reward: -0.9999999962747097, Steps: 30 Mean reward: -0.6212643655017018, Mean steps: 30.0\n",
      "Total num episodes: 120 Success rate: 0.0, Mean reward: -0.6212643655017018, Mean steps: 30.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "agent.eval()  # set to eval mode for deterministic dropout\n",
    "\n",
    "returns_per_episode = np.zeros(n_episode)\n",
    "success_rate = np.zeros(n_episode)\n",
    "# total_steps = np.zeros(n_episode)\n",
    "\n",
    "for task_idx in range(n_episode):\n",
    "    step = 0\n",
    "    running_reward = 0.0\n",
    "    done_rollout = False\n",
    "\n",
    "    if eval_seeds is not None and False:\n",
    "        obs = ptu.from_numpy(env.reset(seed = eval_seeds[task_idx])).to(device)  # reset\n",
    "    else:\n",
    "        obs = ptu.from_numpy(env.reset()).to(device)  # reset\n",
    "\n",
    "    obs = obs.reshape(1, obs.shape[-1])\n",
    "\n",
    "    # assume initial reward = 0.0\n",
    "    action, reward, internal_state = agent.get_initial_info(\n",
    "        config_seq.sampled_seq_len\n",
    "    )\n",
    "\n",
    "    while not done_rollout:\n",
    "        action, internal_state = agent.act(\n",
    "            prev_internal_state=internal_state,\n",
    "            prev_action=action.to(device),\n",
    "            reward=reward.to(device),\n",
    "            obs=obs.to(device),\n",
    "            deterministic=deterministic,\n",
    "        )\n",
    "\n",
    "\n",
    "        # observe reward and next obs\n",
    "        next_obs, reward, done, info = utl.env_step(\n",
    "            env, action.squeeze(dim=0)\n",
    "        )\n",
    "\n",
    "        # add raw reward\n",
    "        running_reward += reward.item()\n",
    "        step += 1\n",
    "        done_rollout = False if ptu.get_numpy(done[0][0]) == 0.0 else True\n",
    "\n",
    "        # set: obs <- next_obs\n",
    "        obs = next_obs.clone()\n",
    "\n",
    "    #returns_per_episode[task_idx] = running_reward\n",
    "    #total_steps[task_idx] = step\n",
    "    if \"success\" in info and info[\"success\"] == True:  # keytodoor\n",
    "        success_rate[task_idx] = 1.0\n",
    "        num_successes += 1\n",
    "    \n",
    "    total_reward += running_reward\n",
    "    total_steps += step\n",
    "\n",
    "    curr_seed = eval_seeds[task_idx]\n",
    "    print(f'Episode: {task_idx}, seed: {curr_seed} Reward: {running_reward}, Steps: {step} Mean reward: {total_reward / (task_idx + 1)}, Mean steps: {total_steps / (task_idx + 1)}')\n",
    "\n",
    "\n",
    "print(f'Total num episodes: {n_episode} Success rate: {num_successes / n_episode}, Mean reward: {total_reward / n_episode}, Mean steps: {total_steps / n_episode}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
