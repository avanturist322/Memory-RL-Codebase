{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "sys.path.append(\"../../models/DTQN\")\n",
    "\n",
    "from environments.Passive_T_Maze_Flag.env.env_passive_t_maze_flag import TMazeClassicPassive\n",
    "from models.DTQN.dtqn.agents.dtqn import DtqnAgent\n",
    "from models.DTQN.utils.agent_utils import get_agent\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Tuple, Any, Callable, Iterator, Set, Optional, overload, TypeVar, Mapping, Dict, List\n",
    "from collections import OrderedDict, namedtuple\n",
    "\n",
    "\n",
    "class _IncompatibleKeys(namedtuple('IncompatibleKeys', ['missing_keys', 'unexpected_keys'])):\n",
    "    def __repr__(self):\n",
    "        if not self.missing_keys and not self.unexpected_keys:\n",
    "            return '<All keys matched successfully>'\n",
    "        return super().__repr__()\n",
    "\n",
    "    __str__ = __repr__\n",
    "\n",
    "def load_state_dict(module, state_dict: Mapping[str, Any],\n",
    "                    strict: bool = True, assign: bool = False):\n",
    "    r\"\"\"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n",
    "\n",
    "    If :attr:`strict` is ``True``, then\n",
    "    the keys of :attr:`state_dict` must exactly match the keys returned\n",
    "    by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
    "\n",
    "    .. warning::\n",
    "        If :attr:`assign` is ``True`` the optimizer must be created after\n",
    "        the call to :attr:`load_state_dict` unless\n",
    "        :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.\n",
    "\n",
    "    Args:\n",
    "        state_dict (dict): a dict containing parameters and\n",
    "            persistent buffers.\n",
    "        strict (bool, optional): whether to strictly enforce that the keys\n",
    "            in :attr:`state_dict` match the keys returned by this module's\n",
    "            :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
    "        assign (bool, optional): When ``False``, the properties of the tensors\n",
    "            in the current module are preserved while when ``True``, the\n",
    "            properties of the Tensors in the state dict are preserved. The only\n",
    "            exception is the ``requires_grad`` field of :class:`~torch.nn.Parameter`s\n",
    "            for which the value from the module is preserved.\n",
    "            Default: ``False``\n",
    "\n",
    "    Returns:\n",
    "        ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
    "            * **missing_keys** is a list of str containing the missing keys\n",
    "            * **unexpected_keys** is a list of str containing the unexpected keys\n",
    "\n",
    "    Note:\n",
    "        If a parameter or buffer is registered as ``None`` and its corresponding key\n",
    "        exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
    "        ``RuntimeError``.\n",
    "    \"\"\"\n",
    "    if not isinstance(state_dict, Mapping):\n",
    "        raise TypeError(f\"Expected state_dict to be dict-like, got {type(state_dict)}.\")\n",
    "\n",
    "    missing_keys: List[str] = []\n",
    "    unexpected_keys: List[str] = []\n",
    "    error_msgs: List[str] = []\n",
    "\n",
    "    # copy state_dict so _load_from_state_dict can modify it\n",
    "    metadata = getattr(state_dict, '_metadata', None)\n",
    "    state_dict = OrderedDict(state_dict)\n",
    "    if metadata is not None:\n",
    "        # mypy isn't aware that \"_metadata\" exists in state_dict\n",
    "        state_dict._metadata = metadata  # type: ignore[attr-defined]\n",
    "\n",
    "\n",
    "    def load(module, local_state_dict, prefix=''):\n",
    "        local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n",
    "        if assign:\n",
    "            local_metadata['assign_to_params_buffers'] = assign\n",
    "        module._load_from_state_dict(\n",
    "            local_state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n",
    "        for name, child in module._modules.items():\n",
    "            if child is not None:\n",
    "                child_prefix = prefix + name + '.'\n",
    "                child_state_dict = {k: v for k, v in local_state_dict.items() if k.startswith(child_prefix)}\n",
    "                load(child, child_state_dict, child_prefix)  # noqa: F821\n",
    "\n",
    "        # Note that the hook can modify missing_keys and unexpected_keys.\n",
    "        incompatible_keys = _IncompatibleKeys(missing_keys, unexpected_keys)\n",
    "        for hook in module._load_state_dict_post_hooks.values():\n",
    "            out = hook(module, incompatible_keys)\n",
    "            assert out is None, (\n",
    "                \"Hooks registered with ``register_load_state_dict_post_hook`` are not\"\n",
    "                \"expected to return new values, if incompatible_keys need to be modified,\"\n",
    "                \"it should be done inplace.\"\n",
    "            )\n",
    "\n",
    "    load(module, state_dict)\n",
    "    del load\n",
    "\n",
    "    if strict:\n",
    "        if len(unexpected_keys) > 0:\n",
    "            error_msgs.insert(\n",
    "                0, 'Unexpected key(s) in state_dict: {}. '.format(\n",
    "                    ', '.join(f'\"{k}\"' for k in unexpected_keys)))\n",
    "        if len(missing_keys) > 0:\n",
    "            error_msgs.insert(\n",
    "                0, 'Missing key(s) in state_dict: {}. '.format(\n",
    "                    ', '.join(f'\"{k}\"' for k in missing_keys)))\n",
    "\n",
    "    if len(error_msgs) > 0:\n",
    "        print('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
    "                        module.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n",
    "        #raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
    "        #                module.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n",
    "    return _IncompatibleKeys(missing_keys, unexpected_keys)\n",
    "\n",
    "from itertools import permutations\n",
    "\n",
    "def generate_permutations(nums):\n",
    "\n",
    "    perms = permutations(nums)\n",
    "    result = [int(''.join(map(str, perm))) for perm in perms]\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = '/opt/Memory-RL-Codebase/configs/DTQN_configs/Passive_T_Maze_Flag/Dense/ICLR_exp_1/Passive_T_Maze_Flag_SHORT_TERM.yaml'\n",
    "\n",
    "\n",
    "\n",
    "episode_timeout = 20\n",
    "corridor_length = episode_timeout - 2\n",
    "penalty = -1/(episode_timeout - 1)\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "\n",
    "with open(config_path, 'r') as file:\n",
    "    args = yaml.safe_load(file)\n",
    "\n",
    "env = TMazeClassicPassive(episode_length=episode_timeout, \n",
    "                            corridor_length=corridor_length, \n",
    "                            goal_reward=1.0,\n",
    "                            penalty=penalty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '/opt/Memory-RL-Codebase/autorun/checkpoints/Passive_T_Maze_Flag/DTQN/DTQN_Passive_T_Maze_Flag_SHORT_TERM_dense/2024_09_28-23_43_40.pt'\n",
    "\n",
    "\n",
    "args['inembed'] = 64\n",
    "args['context'] = episode_timeout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args['pos'] = 'sin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiDiscrete([3 3 2 3], start=[-1 -1  0 -1]) Discrete(4)\n",
      "LayerNorm weigths init!\n",
      "LayerNorm weigths init!\n",
      "LayerNorm weigths init!\n",
      "LayerNorm weigths init!\n",
      "LayerNorm weigths init!\n",
      "LayerNorm weigths init!\n",
      "LayerNorm weigths init!\n",
      "LayerNorm weigths init!\n",
      "LayerNorm weigths init!\n",
      "LayerNorm weigths init!\n",
      "LayerNorm weigths init!\n",
      "LayerNorm weigths init!\n",
      "LayerNorm weigths init!\n",
      "LayerNorm weigths init!\n",
      "LayerNorm weigths init!\n",
      "LayerNorm weigths init!\n",
      "LayerNorm weigths init!\n",
      "LayerNorm weigths init!\n",
      "LayerNorm weigths init!\n",
      "LayerNorm weigths init!\n",
      "LayerNorm weigths init!\n",
      "LayerNorm weigths init!\n",
      "LayerNorm weigths init!\n",
      "LayerNorm weigths init!\n",
      "LayerNorm weigths init!\n",
      "LayerNorm weigths init!\n",
      "LayerNorm weigths init!\n",
      "LayerNorm weigths init!\n",
      "LayerNorm weigths init!\n",
      "LayerNorm weigths init!\n",
      "LayerNorm weigths init!\n",
      "LayerNorm weigths init!\n",
      "LayerNorm weigths init!\n",
      "LayerNorm weigths init!\n",
      "MultiDiscrete([3 3 2 3], start=[-1 -1  0 -1]) Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "agent = get_agent(\n",
    "        args['model'],\n",
    "        env,\n",
    "        env,\n",
    "        args['obsembed'],\n",
    "        args['inembed'],\n",
    "        args['buf_size'],\n",
    "        device,\n",
    "        args['lr'],\n",
    "        args['batch'],\n",
    "        args['context'],\n",
    "        args['history'],\n",
    "        args['num_steps'],\n",
    "        # DTQN specific\n",
    "        args['heads'],\n",
    "        args['layers'],\n",
    "        args['dropout'],\n",
    "        args['identity'],\n",
    "        args['gate'],\n",
    "        args['pos'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# att_mask = agent.policy_network.transformer_layers[1].attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['position_embedding', 'obs_embedding.embedding.weight', 'obs_embedding.embedding.bias', 'transformer_layers.0.attn_mask', 'transformer_layers.0.layernorm1.weight', 'transformer_layers.0.layernorm1.bias', 'transformer_layers.0.layernorm2.weight', 'transformer_layers.0.layernorm2.bias', 'transformer_layers.0.attention.in_proj_weight', 'transformer_layers.0.attention.in_proj_bias', 'transformer_layers.0.attention.out_proj.weight', 'transformer_layers.0.attention.out_proj.bias', 'transformer_layers.0.ffn.0.weight', 'transformer_layers.0.ffn.0.bias', 'transformer_layers.0.ffn.2.weight', 'transformer_layers.0.ffn.2.bias', 'transformer_layers.1.attn_mask', 'transformer_layers.1.layernorm1.weight', 'transformer_layers.1.layernorm1.bias', 'transformer_layers.1.layernorm2.weight', 'transformer_layers.1.layernorm2.bias', 'transformer_layers.1.attention.in_proj_weight', 'transformer_layers.1.attention.in_proj_bias', 'transformer_layers.1.attention.out_proj.weight', 'transformer_layers.1.attention.out_proj.bias', 'transformer_layers.1.ffn.0.weight', 'transformer_layers.1.ffn.0.bias', 'transformer_layers.1.ffn.2.weight', 'transformer_layers.1.ffn.2.bias', 'transformer_layers.2.attn_mask', 'transformer_layers.2.layernorm1.weight', 'transformer_layers.2.layernorm1.bias', 'transformer_layers.2.layernorm2.weight', 'transformer_layers.2.layernorm2.bias', 'transformer_layers.2.attention.in_proj_weight', 'transformer_layers.2.attention.in_proj_bias', 'transformer_layers.2.attention.out_proj.weight', 'transformer_layers.2.attention.out_proj.bias', 'transformer_layers.2.ffn.0.weight', 'transformer_layers.2.ffn.0.bias', 'transformer_layers.2.ffn.2.weight', 'transformer_layers.2.ffn.2.bias', 'transformer_layers.3.attn_mask', 'transformer_layers.3.layernorm1.weight', 'transformer_layers.3.layernorm1.bias', 'transformer_layers.3.layernorm2.weight', 'transformer_layers.3.layernorm2.bias', 'transformer_layers.3.attention.in_proj_weight', 'transformer_layers.3.attention.in_proj_bias', 'transformer_layers.3.attention.out_proj.weight', 'transformer_layers.3.attention.out_proj.bias', 'transformer_layers.3.ffn.0.weight', 'transformer_layers.3.ffn.0.bias', 'transformer_layers.3.ffn.2.weight', 'transformer_layers.3.ffn.2.bias', 'transformer_layers.4.attn_mask', 'transformer_layers.4.layernorm1.weight', 'transformer_layers.4.layernorm1.bias', 'transformer_layers.4.layernorm2.weight', 'transformer_layers.4.layernorm2.bias', 'transformer_layers.4.attention.in_proj_weight', 'transformer_layers.4.attention.in_proj_bias', 'transformer_layers.4.attention.out_proj.weight', 'transformer_layers.4.attention.out_proj.bias', 'transformer_layers.4.ffn.0.weight', 'transformer_layers.4.ffn.0.bias', 'transformer_layers.4.ffn.2.weight', 'transformer_layers.4.ffn.2.bias', 'transformer_layers.5.attn_mask', 'transformer_layers.5.layernorm1.weight', 'transformer_layers.5.layernorm1.bias', 'transformer_layers.5.layernorm2.weight', 'transformer_layers.5.layernorm2.bias', 'transformer_layers.5.attention.in_proj_weight', 'transformer_layers.5.attention.in_proj_bias', 'transformer_layers.5.attention.out_proj.weight', 'transformer_layers.5.attention.out_proj.bias', 'transformer_layers.5.ffn.0.weight', 'transformer_layers.5.ffn.0.bias', 'transformer_layers.5.ffn.2.weight', 'transformer_layers.5.ffn.2.bias', 'transformer_layers.6.attn_mask', 'transformer_layers.6.layernorm1.weight', 'transformer_layers.6.layernorm1.bias', 'transformer_layers.6.layernorm2.weight', 'transformer_layers.6.layernorm2.bias', 'transformer_layers.6.attention.in_proj_weight', 'transformer_layers.6.attention.in_proj_bias', 'transformer_layers.6.attention.out_proj.weight', 'transformer_layers.6.attention.out_proj.bias', 'transformer_layers.6.ffn.0.weight', 'transformer_layers.6.ffn.0.bias', 'transformer_layers.6.ffn.2.weight', 'transformer_layers.6.ffn.2.bias', 'transformer_layers.7.attn_mask', 'transformer_layers.7.layernorm1.weight', 'transformer_layers.7.layernorm1.bias', 'transformer_layers.7.layernorm2.weight', 'transformer_layers.7.layernorm2.bias', 'transformer_layers.7.attention.in_proj_weight', 'transformer_layers.7.attention.in_proj_bias', 'transformer_layers.7.attention.out_proj.weight', 'transformer_layers.7.attention.out_proj.bias', 'transformer_layers.7.ffn.0.weight', 'transformer_layers.7.ffn.0.bias', 'transformer_layers.7.ffn.2.weight', 'transformer_layers.7.ffn.2.bias', 'layernorm.weight', 'layernorm.bias', 'ffn.0.weight', 'ffn.0.bias', 'ffn.2.weight', 'ffn.2.bias'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.policy_network.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckp  = torch.load(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['position_embedding', 'obs_embedding.embedding.weight', 'obs_embedding.embedding.bias', 'transformer_layers.0.attn_mask', 'transformer_layers.0.layernorm1.weight', 'transformer_layers.0.layernorm1.bias', 'transformer_layers.0.layernorm2.weight', 'transformer_layers.0.layernorm2.bias', 'transformer_layers.0.attention.in_proj_weight', 'transformer_layers.0.attention.in_proj_bias', 'transformer_layers.0.attention.out_proj.weight', 'transformer_layers.0.attention.out_proj.bias', 'transformer_layers.0.ffn.0.weight', 'transformer_layers.0.ffn.0.bias', 'transformer_layers.0.ffn.2.weight', 'transformer_layers.0.ffn.2.bias', 'transformer_layers.1.attn_mask', 'transformer_layers.1.layernorm1.weight', 'transformer_layers.1.layernorm1.bias', 'transformer_layers.1.layernorm2.weight', 'transformer_layers.1.layernorm2.bias', 'transformer_layers.1.attention.in_proj_weight', 'transformer_layers.1.attention.in_proj_bias', 'transformer_layers.1.attention.out_proj.weight', 'transformer_layers.1.attention.out_proj.bias', 'transformer_layers.1.ffn.0.weight', 'transformer_layers.1.ffn.0.bias', 'transformer_layers.1.ffn.2.weight', 'transformer_layers.1.ffn.2.bias', 'transformer_layers.2.attn_mask', 'transformer_layers.2.layernorm1.weight', 'transformer_layers.2.layernorm1.bias', 'transformer_layers.2.layernorm2.weight', 'transformer_layers.2.layernorm2.bias', 'transformer_layers.2.attention.in_proj_weight', 'transformer_layers.2.attention.in_proj_bias', 'transformer_layers.2.attention.out_proj.weight', 'transformer_layers.2.attention.out_proj.bias', 'transformer_layers.2.ffn.0.weight', 'transformer_layers.2.ffn.0.bias', 'transformer_layers.2.ffn.2.weight', 'transformer_layers.2.ffn.2.bias', 'transformer_layers.3.attn_mask', 'transformer_layers.3.layernorm1.weight', 'transformer_layers.3.layernorm1.bias', 'transformer_layers.3.layernorm2.weight', 'transformer_layers.3.layernorm2.bias', 'transformer_layers.3.attention.in_proj_weight', 'transformer_layers.3.attention.in_proj_bias', 'transformer_layers.3.attention.out_proj.weight', 'transformer_layers.3.attention.out_proj.bias', 'transformer_layers.3.ffn.0.weight', 'transformer_layers.3.ffn.0.bias', 'transformer_layers.3.ffn.2.weight', 'transformer_layers.3.ffn.2.bias', 'transformer_layers.4.attn_mask', 'transformer_layers.4.layernorm1.weight', 'transformer_layers.4.layernorm1.bias', 'transformer_layers.4.layernorm2.weight', 'transformer_layers.4.layernorm2.bias', 'transformer_layers.4.attention.in_proj_weight', 'transformer_layers.4.attention.in_proj_bias', 'transformer_layers.4.attention.out_proj.weight', 'transformer_layers.4.attention.out_proj.bias', 'transformer_layers.4.ffn.0.weight', 'transformer_layers.4.ffn.0.bias', 'transformer_layers.4.ffn.2.weight', 'transformer_layers.4.ffn.2.bias', 'transformer_layers.5.attn_mask', 'transformer_layers.5.layernorm1.weight', 'transformer_layers.5.layernorm1.bias', 'transformer_layers.5.layernorm2.weight', 'transformer_layers.5.layernorm2.bias', 'transformer_layers.5.attention.in_proj_weight', 'transformer_layers.5.attention.in_proj_bias', 'transformer_layers.5.attention.out_proj.weight', 'transformer_layers.5.attention.out_proj.bias', 'transformer_layers.5.ffn.0.weight', 'transformer_layers.5.ffn.0.bias', 'transformer_layers.5.ffn.2.weight', 'transformer_layers.5.ffn.2.bias', 'transformer_layers.6.attn_mask', 'transformer_layers.6.layernorm1.weight', 'transformer_layers.6.layernorm1.bias', 'transformer_layers.6.layernorm2.weight', 'transformer_layers.6.layernorm2.bias', 'transformer_layers.6.attention.in_proj_weight', 'transformer_layers.6.attention.in_proj_bias', 'transformer_layers.6.attention.out_proj.weight', 'transformer_layers.6.attention.out_proj.bias', 'transformer_layers.6.ffn.0.weight', 'transformer_layers.6.ffn.0.bias', 'transformer_layers.6.ffn.2.weight', 'transformer_layers.6.ffn.2.bias', 'transformer_layers.7.attn_mask', 'transformer_layers.7.layernorm1.weight', 'transformer_layers.7.layernorm1.bias', 'transformer_layers.7.layernorm2.weight', 'transformer_layers.7.layernorm2.bias', 'transformer_layers.7.attention.in_proj_weight', 'transformer_layers.7.attention.in_proj_bias', 'transformer_layers.7.attention.out_proj.weight', 'transformer_layers.7.attention.out_proj.bias', 'transformer_layers.7.ffn.0.weight', 'transformer_layers.7.ffn.0.bias', 'transformer_layers.7.ffn.2.weight', 'transformer_layers.7.ffn.2.bias', 'layernorm.weight', 'layernorm.bias', 'ffn.0.weight', 'ffn.0.bias', 'ffn.2.weight', 'ffn.2.bias'])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckp['policy_net_state_dict'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 15, 64])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckp['policy_net_state_dict']['position_embedding'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 64])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckp['policy_net_state_dict']['position_embedding'][:, :12, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0, device='cuda:0')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ckp['policy_net_state_dict']['obs_embedding.embedding.weight'] != agent.policy_network.obs_embedding.embedding.weight).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 64])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.policy_network.position_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.policy_network.position_embedding = torch.nn.Parameter(ckp['policy_net_state_dict']['position_embedding'][:, :13, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tensorboard'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.load_checkpoint(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) in loading state_dict for DTQN:\n",
      "\tsize mismatch for position_embedding: copying a param with shape torch.Size([1, 15, 64]) from checkpoint, the shape in current model is torch.Size([1, 20, 64]).\n",
      "\tsize mismatch for transformer_layers.0.attn_mask: copying a param with shape torch.Size([15, 15]) from checkpoint, the shape in current model is torch.Size([20, 20]).\n",
      "\tsize mismatch for transformer_layers.1.attn_mask: copying a param with shape torch.Size([15, 15]) from checkpoint, the shape in current model is torch.Size([20, 20]).\n",
      "\tsize mismatch for transformer_layers.2.attn_mask: copying a param with shape torch.Size([15, 15]) from checkpoint, the shape in current model is torch.Size([20, 20]).\n",
      "\tsize mismatch for transformer_layers.3.attn_mask: copying a param with shape torch.Size([15, 15]) from checkpoint, the shape in current model is torch.Size([20, 20]).\n",
      "\tsize mismatch for transformer_layers.4.attn_mask: copying a param with shape torch.Size([15, 15]) from checkpoint, the shape in current model is torch.Size([20, 20]).\n",
      "\tsize mismatch for transformer_layers.5.attn_mask: copying a param with shape torch.Size([15, 15]) from checkpoint, the shape in current model is torch.Size([20, 20]).\n",
      "\tsize mismatch for transformer_layers.6.attn_mask: copying a param with shape torch.Size([15, 15]) from checkpoint, the shape in current model is torch.Size([20, 20]).\n",
      "\tsize mismatch for transformer_layers.7.attn_mask: copying a param with shape torch.Size([15, 15]) from checkpoint, the shape in current model is torch.Size([20, 20]).\n",
      "Error(s) in loading state_dict for DTQN:\n",
      "\tsize mismatch for position_embedding: copying a param with shape torch.Size([1, 15, 64]) from checkpoint, the shape in current model is torch.Size([1, 20, 64]).\n",
      "\tsize mismatch for transformer_layers.0.attn_mask: copying a param with shape torch.Size([15, 15]) from checkpoint, the shape in current model is torch.Size([20, 20]).\n",
      "\tsize mismatch for transformer_layers.1.attn_mask: copying a param with shape torch.Size([15, 15]) from checkpoint, the shape in current model is torch.Size([20, 20]).\n",
      "\tsize mismatch for transformer_layers.2.attn_mask: copying a param with shape torch.Size([15, 15]) from checkpoint, the shape in current model is torch.Size([20, 20]).\n",
      "\tsize mismatch for transformer_layers.3.attn_mask: copying a param with shape torch.Size([15, 15]) from checkpoint, the shape in current model is torch.Size([20, 20]).\n",
      "\tsize mismatch for transformer_layers.4.attn_mask: copying a param with shape torch.Size([15, 15]) from checkpoint, the shape in current model is torch.Size([20, 20]).\n",
      "\tsize mismatch for transformer_layers.5.attn_mask: copying a param with shape torch.Size([15, 15]) from checkpoint, the shape in current model is torch.Size([20, 20]).\n",
      "\tsize mismatch for transformer_layers.6.attn_mask: copying a param with shape torch.Size([15, 15]) from checkpoint, the shape in current model is torch.Size([20, 20]).\n",
      "\tsize mismatch for transformer_layers.7.attn_mask: copying a param with shape torch.Size([15, 15]) from checkpoint, the shape in current model is torch.Size([20, 20]).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_state_dict(agent.policy_network, ckp['policy_net_state_dict'])\n",
    "load_state_dict(agent.target_network, ckp['target_net_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 12])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.policy_network.transformer_layers[1].attn_mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nums = [1, 2, 3, 4, 5]\n",
    "eval_seeds = generate_permutations(nums)\n",
    "\n",
    "videos_limit = len(eval_seeds) + 1\n",
    "n_episode = len(eval_seeds)\n",
    "\n",
    "videos_dir = '/opt/Memory-RL-Codebase/eval/Minigrid_Memory/DTQN'\n",
    "\n",
    "run_name = checkpoint_path.split('/')[-1].strip('.pt')\n",
    "run_type = checkpoint_path.split('/')[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate!\n",
      "Episode: 0, seed: 12345 Reward: -0.05263157894736842, Steps: 20 Mean reward: -0.05263157894736842, Mean steps: 20.0\n",
      "Episode: 1, seed: 12354 Reward: -0.05263157894736842, Steps: 20 Mean reward: -0.05263157894736842, Mean steps: 20.0\n",
      "Episode: 2, seed: 12435 Reward: -0.05263157894736842, Steps: 20 Mean reward: -0.05263157894736842, Mean steps: 20.0\n",
      "Episode: 3, seed: 12453 Reward: -0.05263157894736842, Steps: 20 Mean reward: -0.05263157894736842, Mean steps: 20.0\n",
      "Episode: 4, seed: 12534 Reward: -0.05263157894736842, Steps: 20 Mean reward: -0.05263157894736842, Mean steps: 20.0\n",
      "Episode: 5, seed: 12543 Reward: -0.05263157894736842, Steps: 20 Mean reward: -0.05263157894736842, Mean steps: 20.0\n",
      "Episode: 6, seed: 13245 Reward: -0.05263157894736842, Steps: 20 Mean reward: -0.05263157894736842, Mean steps: 20.0\n",
      "Episode: 7, seed: 13254 Reward: -0.05263157894736842, Steps: 20 Mean reward: -0.05263157894736842, Mean steps: 20.0\n",
      "Episode: 8, seed: 13425 Reward: 1.0, Steps: 19 Mean reward: 0.06432748538011696, Mean steps: 19.88888888888889\n",
      "Episode: 9, seed: 13452 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.05263157894736843, Mean steps: 19.9\n",
      "Episode: 10, seed: 13524 Reward: 1.0, Steps: 19 Mean reward: 0.13875598086124402, Mean steps: 19.818181818181817\n",
      "Episode: 11, seed: 13542 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.12280701754385966, Mean steps: 19.833333333333332\n",
      "Episode: 12, seed: 14235 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.10931174089068828, Mean steps: 19.846153846153847\n",
      "Episode: 13, seed: 14253 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.09774436090225566, Mean steps: 19.857142857142858\n",
      "Episode: 14, seed: 14325 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.08771929824561406, Mean steps: 19.866666666666667\n",
      "Episode: 15, seed: 14352 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.07894736842105265, Mean steps: 19.875\n",
      "Episode: 16, seed: 14523 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.0712074303405573, Mean steps: 19.88235294117647\n",
      "Episode: 17, seed: 14532 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.06432748538011698, Mean steps: 19.88888888888889\n",
      "Episode: 18, seed: 15234 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.0581717451523546, Mean steps: 19.894736842105264\n",
      "Episode: 19, seed: 15243 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.05263157894736845, Mean steps: 19.9\n",
      "Episode: 20, seed: 15324 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.04761904761904765, Mean steps: 19.904761904761905\n",
      "Episode: 21, seed: 15342 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.04306220095693783, Mean steps: 19.90909090909091\n",
      "Episode: 22, seed: 15423 Reward: 1.0, Steps: 19 Mean reward: 0.08466819221967967, Mean steps: 19.869565217391305\n",
      "Episode: 23, seed: 15432 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.07894736842105267, Mean steps: 19.875\n",
      "Episode: 24, seed: 21345 Reward: 1.0, Steps: 19 Mean reward: 0.11578947368421055, Mean steps: 19.84\n",
      "Episode: 25, seed: 21354 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.10931174089068828, Mean steps: 19.846153846153847\n",
      "Episode: 26, seed: 21435 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.10331384015594543, Mean steps: 19.85185185185185\n",
      "Episode: 27, seed: 21453 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.09774436090225565, Mean steps: 19.857142857142858\n",
      "Episode: 28, seed: 21534 Reward: 1.0, Steps: 19 Mean reward: 0.12885662431941924, Mean steps: 19.82758620689655\n",
      "Episode: 29, seed: 21543 Reward: 1.0, Steps: 19 Mean reward: 0.15789473684210525, Mean steps: 19.8\n",
      "Episode: 30, seed: 23145 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.15110356536502548, Mean steps: 19.806451612903224\n",
      "Episode: 31, seed: 23154 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.14473684210526316, Mean steps: 19.8125\n",
      "Episode: 32, seed: 23415 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.13875598086124402, Mean steps: 19.818181818181817\n",
      "Episode: 33, seed: 23451 Reward: 1.0, Steps: 19 Mean reward: 0.16408668730650156, Mean steps: 19.794117647058822\n",
      "Episode: 34, seed: 23514 Reward: 1.0, Steps: 19 Mean reward: 0.1879699248120301, Mean steps: 19.771428571428572\n",
      "Episode: 35, seed: 23541 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.18128654970760236, Mean steps: 19.77777777777778\n",
      "Episode: 36, seed: 24135 Reward: 1.0, Steps: 19 Mean reward: 0.20341394025604553, Mean steps: 19.756756756756758\n",
      "Episode: 37, seed: 24153 Reward: -0.10526315789473684, Steps: 20 Mean reward: 0.1952908587257618, Mean steps: 19.763157894736842\n",
      "Episode: 38, seed: 24315 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.18893387314439947, Mean steps: 19.76923076923077\n",
      "Episode: 39, seed: 24351 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.18289473684210528, Mean steps: 19.775\n",
      "Episode: 40, seed: 24513 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.17715019255455716, Mean steps: 19.78048780487805\n",
      "Episode: 41, seed: 24531 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.17167919799498751, Mean steps: 19.785714285714285\n",
      "Episode: 42, seed: 25134 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.16646266829865367, Mean steps: 19.790697674418606\n",
      "Episode: 43, seed: 25143 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.16148325358851678, Mean steps: 19.795454545454547\n",
      "Episode: 44, seed: 25314 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.15672514619883046, Mean steps: 19.8\n",
      "Episode: 45, seed: 25341 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.15217391304347833, Mean steps: 19.804347826086957\n",
      "Episode: 46, seed: 25413 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.1478163493840986, Mean steps: 19.80851063829787\n",
      "Episode: 47, seed: 25431 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.14364035087719304, Mean steps: 19.8125\n",
      "Episode: 48, seed: 31245 Reward: 1.0, Steps: 19 Mean reward: 0.16111707841031156, Mean steps: 19.79591836734694\n",
      "Episode: 49, seed: 31254 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.15684210526315798, Mean steps: 19.8\n",
      "Episode: 50, seed: 31425 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.1527347781217751, Mean steps: 19.80392156862745\n",
      "Episode: 51, seed: 31452 Reward: 1.0, Steps: 19 Mean reward: 0.16902834008097176, Mean steps: 19.78846153846154\n",
      "Episode: 52, seed: 31524 Reward: 1.0, Steps: 19 Mean reward: 0.1847070506454817, Mean steps: 19.77358490566038\n",
      "Episode: 53, seed: 31542 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.18031189083820673, Mean steps: 19.77777777777778\n",
      "Episode: 54, seed: 32145 Reward: 1.0, Steps: 19 Mean reward: 0.1952153110047848, Mean steps: 19.763636363636362\n",
      "Episode: 55, seed: 32154 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.19078947368421062, Mean steps: 19.767857142857142\n",
      "Episode: 56, seed: 32415 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.18651892890120048, Mean steps: 19.771929824561404\n",
      "Episode: 57, seed: 32451 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.1823956442831217, Mean steps: 19.775862068965516\n",
      "Episode: 58, seed: 32514 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.1784121320249778, Mean steps: 19.779661016949152\n",
      "Episode: 59, seed: 32541 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.17456140350877203, Mean steps: 19.783333333333335\n",
      "Episode: 60, seed: 34125 Reward: -0.7894736842105261, Steps: 20 Mean reward: 0.15875754961173436, Mean steps: 19.78688524590164\n",
      "Episode: 61, seed: 34152 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.15534804753820045, Mean steps: 19.79032258064516\n",
      "Episode: 62, seed: 34215 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.15204678362573112, Mean steps: 19.793650793650794\n",
      "Episode: 63, seed: 34251 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.14884868421052644, Mean steps: 19.796875\n",
      "Episode: 64, seed: 34512 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.14574898785425114, Mean steps: 19.8\n",
      "Episode: 65, seed: 34521 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.14274322169059023, Mean steps: 19.803030303030305\n",
      "Episode: 66, seed: 35124 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.1398271798900237, Mean steps: 19.80597014925373\n",
      "Episode: 67, seed: 35142 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.13699690402476794, Mean steps: 19.808823529411764\n",
      "Episode: 68, seed: 35214 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.13424866514111378, Mean steps: 19.81159420289855\n",
      "Episode: 69, seed: 35241 Reward: 1.0, Steps: 19 Mean reward: 0.1466165413533836, Mean steps: 19.8\n",
      "Episode: 70, seed: 35412 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.14381022979985186, Mean steps: 19.802816901408452\n",
      "Episode: 71, seed: 35421 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.14108187134502936, Mean steps: 19.805555555555557\n",
      "Episode: 72, seed: 41235 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.13842826243691433, Mean steps: 19.80821917808219\n",
      "Episode: 73, seed: 41253 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.1358463726884781, Mean steps: 19.81081081081081\n",
      "Episode: 74, seed: 41325 Reward: 1.0, Steps: 19 Mean reward: 0.1473684210526317, Mean steps: 19.8\n",
      "Episode: 75, seed: 41352 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.1447368421052633, Mean steps: 19.80263157894737\n",
      "Episode: 76, seed: 41523 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.14217361585782654, Mean steps: 19.805194805194805\n",
      "Episode: 77, seed: 41532 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.13967611336032404, Mean steps: 19.807692307692307\n",
      "Episode: 78, seed: 42135 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.13724183877415072, Mean steps: 19.810126582278482\n",
      "Episode: 79, seed: 42153 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.13486842105263172, Mean steps: 19.8125\n",
      "Episode: 80, seed: 42315 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.13255360623781692, Mean steps: 19.814814814814813\n",
      "Episode: 81, seed: 42351 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.1302952503209244, Mean steps: 19.817073170731707\n",
      "Episode: 82, seed: 42513 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.12809131261889678, Mean steps: 19.819277108433734\n",
      "Episode: 83, seed: 42531 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.1259398496240603, Mean steps: 19.821428571428573\n",
      "Episode: 84, seed: 43125 Reward: 0.9473684210526316, Steps: 20 Mean reward: 0.1356037151702788, Mean steps: 19.823529411764707\n",
      "Episode: 85, seed: 43152 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.13341493268053872, Mean steps: 19.825581395348838\n",
      "Episode: 86, seed: 43215 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.13127646702964324, Mean steps: 19.82758620689655\n",
      "Episode: 87, seed: 43251 Reward: 1.0, Steps: 19 Mean reward: 0.14114832535885183, Mean steps: 19.818181818181817\n",
      "Episode: 88, seed: 43512 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.13897102306327633, Mean steps: 19.820224719101123\n",
      "Episode: 89, seed: 43521 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.13684210526315804, Mean steps: 19.822222222222223\n",
      "Episode: 90, seed: 45123 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.1347599768652402, Mean steps: 19.824175824175825\n",
      "Episode: 91, seed: 45132 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.1327231121281466, Mean steps: 19.82608695652174\n",
      "Episode: 92, seed: 45213 Reward: -0.6842105263157894, Steps: 20 Mean reward: 0.12393887945670644, Mean steps: 19.827956989247312\n",
      "Episode: 93, seed: 45231 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.1220604703247482, Mean steps: 19.829787234042552\n",
      "Episode: 94, seed: 45312 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.12022160664819961, Mean steps: 19.83157894736842\n",
      "Episode: 95, seed: 45321 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.11842105263157911, Mean steps: 19.833333333333332\n",
      "Episode: 96, seed: 51234 Reward: 1.0, Steps: 19 Mean reward: 0.12750949538795459, Mean steps: 19.824742268041238\n",
      "Episode: 97, seed: 51243 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.12567132116004312, Mean steps: 19.8265306122449\n",
      "Episode: 98, seed: 51324 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.12387028176501877, Mean steps: 19.828282828282827\n",
      "Episode: 99, seed: 51342 Reward: 0.9473684210526316, Steps: 20 Mean reward: 0.1321052631578949, Mean steps: 19.83\n",
      "Episode: 100, seed: 51423 Reward: 1.0, Steps: 19 Mean reward: 0.1406982803543514, Mean steps: 19.821782178217823\n",
      "Episode: 101, seed: 51432 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.13880288957688355, Mean steps: 19.823529411764707\n",
      "Episode: 102, seed: 52134 Reward: 0.9473684210526316, Steps: 20 Mean reward: 0.14665304036791024, Mean steps: 19.825242718446603\n",
      "Episode: 103, seed: 52143 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.14473684210526333, Mean steps: 19.826923076923077\n",
      "Episode: 104, seed: 52314 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.14285714285714302, Mean steps: 19.82857142857143\n",
      "Episode: 105, seed: 52341 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.14101290963257215, Mean steps: 19.830188679245282\n",
      "Episode: 106, seed: 52413 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.13920314805705872, Mean steps: 19.83177570093458\n",
      "Episode: 107, seed: 52431 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.13742690058479548, Mean steps: 19.833333333333332\n",
      "Episode: 108, seed: 53124 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.13568324480927105, Mean steps: 19.834862385321102\n",
      "Episode: 109, seed: 53142 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.13397129186602888, Mean steps: 19.836363636363636\n",
      "Episode: 110, seed: 53214 Reward: 1.0, Steps: 19 Mean reward: 0.14177335229966825, Mean steps: 19.82882882882883\n",
      "Episode: 111, seed: 53241 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.14003759398496257, Mean steps: 19.830357142857142\n",
      "Episode: 112, seed: 53412 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.13833255705635789, Mean steps: 19.831858407079647\n",
      "Episode: 113, seed: 53421 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.1366574330563252, Mean steps: 19.833333333333332\n",
      "Episode: 114, seed: 54123 Reward: 1.0, Steps: 19 Mean reward: 0.14416475972540063, Mean steps: 19.82608695652174\n",
      "Episode: 115, seed: 54132 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.14246823956442847, Mean steps: 19.82758620689655\n",
      "Episode: 116, seed: 54213 Reward: 1.0, Steps: 19 Mean reward: 0.1497975708502026, Mean steps: 19.82051282051282\n",
      "Episode: 117, seed: 54231 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.14808206958073164, Mean steps: 19.822033898305083\n",
      "Episode: 118, seed: 54312 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.14639540026536943, Mean steps: 19.823529411764707\n",
      "Episode: 119, seed: 54321 Reward: -0.05263157894736842, Steps: 20 Mean reward: 0.14473684210526327, Mean steps: 19.825\n",
      "Total num episodes: 120 Success rate: 0.2, Mean reward: 0.14473684210526327, Mean steps: 19.825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2, 0.14473684210526327, 19.825)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.evaluate(n_episode = n_episode, eval_seeds = eval_seeds, render = False, videos_limit = videos_limit, videos_dir = videos_dir, run_name = run_name, run_type =run_type )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.evaluate(n_episode = n_episode, eval_seeds = eval_seeds, render = False, videos_limit = videos_limit, videos_dir = videos_dir, run_name = run_name, run_type =run_type )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate!\n",
      "Episode: 0, seed: 12345 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9090909090909093, Mean steps: 12.0\n",
      "Episode: 1, seed: 12354 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9545454545454548, Mean steps: 12.0\n",
      "Episode: 2, seed: 12435 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9696969696969701, Mean steps: 12.0\n",
      "Episode: 3, seed: 12453 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9772727272727275, Mean steps: 12.0\n",
      "Episode: 4, seed: 12534 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.981818181818182, Mean steps: 12.0\n",
      "Episode: 5, seed: 12543 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9696969696969698, Mean steps: 12.0\n",
      "Episode: 6, seed: 13245 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9610389610389612, Mean steps: 12.0\n",
      "Episode: 7, seed: 13254 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.965909090909091, Mean steps: 12.0\n",
      "Episode: 8, seed: 13425 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9595959595959598, Mean steps: 12.0\n",
      "Episode: 9, seed: 13452 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9636363636363638, Mean steps: 12.0\n",
      "Episode: 10, seed: 13524 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9586776859504135, Mean steps: 12.0\n",
      "Episode: 11, seed: 13542 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9621212121212124, Mean steps: 12.0\n",
      "Episode: 12, seed: 14235 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9580419580419584, Mean steps: 12.0\n",
      "Episode: 13, seed: 14253 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9545454545454549, Mean steps: 12.0\n",
      "Episode: 14, seed: 14325 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9575757575757579, Mean steps: 12.0\n",
      "Episode: 15, seed: 14352 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.960227272727273, Mean steps: 12.0\n",
      "Episode: 16, seed: 14523 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9625668449197865, Mean steps: 12.0\n",
      "Episode: 17, seed: 14532 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.95959595959596, Mean steps: 12.0\n",
      "Episode: 18, seed: 15234 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9617224880382779, Mean steps: 12.0\n",
      "Episode: 19, seed: 15243 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9636363636363641, Mean steps: 12.0\n",
      "Episode: 20, seed: 15324 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9653679653679658, Mean steps: 12.0\n",
      "Episode: 21, seed: 15342 Reward: -0.8181818181818183, Steps: 12 Mean reward: -0.9586776859504137, Mean steps: 12.0\n",
      "Episode: 22, seed: 15423 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9565217391304353, Mean steps: 12.0\n",
      "Episode: 23, seed: 15432 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.954545454545455, Mean steps: 12.0\n",
      "Episode: 24, seed: 21345 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9527272727272732, Mean steps: 12.0\n",
      "Episode: 25, seed: 21354 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.954545454545455, Mean steps: 12.0\n",
      "Episode: 26, seed: 21435 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9562289562289567, Mean steps: 12.0\n",
      "Episode: 27, seed: 21453 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9577922077922082, Mean steps: 12.0\n",
      "Episode: 28, seed: 21534 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9561128526645772, Mean steps: 12.0\n",
      "Episode: 29, seed: 21543 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9575757575757581, Mean steps: 12.0\n",
      "Episode: 30, seed: 23145 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9589442815249272, Mean steps: 12.0\n",
      "Episode: 31, seed: 23154 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9573863636363641, Mean steps: 12.0\n",
      "Episode: 32, seed: 23415 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9559228650137745, Mean steps: 12.0\n",
      "Episode: 33, seed: 23451 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9545454545454549, Mean steps: 12.0\n",
      "Episode: 34, seed: 23514 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9532467532467536, Mean steps: 12.0\n",
      "Episode: 35, seed: 23541 Reward: -0.7272727272727274, Steps: 12 Mean reward: -0.9469696969696972, Mean steps: 12.0\n",
      "Episode: 36, seed: 24135 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9459459459459462, Mean steps: 12.0\n",
      "Episode: 37, seed: 24153 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.944976076555024, Mean steps: 12.0\n",
      "Episode: 38, seed: 24315 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9463869463869465, Mean steps: 12.0\n",
      "Episode: 39, seed: 24351 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9454545454545455, Mean steps: 12.0\n",
      "Episode: 40, seed: 24513 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9467849223946785, Mean steps: 12.0\n",
      "Episode: 41, seed: 24531 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9458874458874459, Mean steps: 12.0\n",
      "Episode: 42, seed: 25134 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9450317124735729, Mean steps: 12.0\n",
      "Episode: 43, seed: 25143 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9462809917355371, Mean steps: 12.0\n",
      "Episode: 44, seed: 25314 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9454545454545453, Mean steps: 12.0\n",
      "Episode: 45, seed: 25341 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9446640316205531, Mean steps: 12.0\n",
      "Episode: 46, seed: 25413 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9458413926499031, Mean steps: 12.0\n",
      "Episode: 47, seed: 25431 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9469696969696968, Mean steps: 12.0\n",
      "Episode: 48, seed: 31245 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9480519480519479, Mean steps: 12.0\n",
      "Episode: 49, seed: 31254 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9490909090909089, Mean steps: 12.0\n",
      "Episode: 50, seed: 31425 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9500891265597147, Mean steps: 12.0\n",
      "Episode: 51, seed: 31452 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.949300699300699, Mean steps: 12.0\n",
      "Episode: 52, seed: 31524 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9485420240137219, Mean steps: 12.0\n",
      "Episode: 53, seed: 31542 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9494949494949493, Mean steps: 12.0\n",
      "Episode: 54, seed: 32145 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9487603305785121, Mean steps: 12.0\n",
      "Episode: 55, seed: 32154 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9496753246753243, Mean steps: 12.0\n",
      "Episode: 56, seed: 32415 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9489633173843697, Mean steps: 12.0\n",
      "Episode: 57, seed: 32451 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9498432601880874, Mean steps: 12.0\n",
      "Episode: 58, seed: 32514 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.949152542372881, Mean steps: 12.0\n",
      "Episode: 59, seed: 32541 Reward: -0.8181818181818183, Steps: 12 Mean reward: -0.9469696969696967, Mean steps: 12.0\n",
      "Episode: 60, seed: 34125 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9463487332339787, Mean steps: 12.0\n",
      "Episode: 61, seed: 34152 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9472140762463339, Mean steps: 12.0\n",
      "Episode: 62, seed: 34215 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9480519480519477, Mean steps: 12.0\n",
      "Episode: 63, seed: 34251 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9474431818181814, Mean steps: 12.0\n",
      "Episode: 64, seed: 34512 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9468531468531465, Mean steps: 12.0\n",
      "Episode: 65, seed: 34521 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9476584022038563, Mean steps: 12.0\n",
      "Episode: 66, seed: 35124 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9484396200814107, Mean steps: 12.0\n",
      "Episode: 67, seed: 35142 Reward: -0.8181818181818183, Steps: 12 Mean reward: -0.9465240641711226, Mean steps: 12.0\n",
      "Episode: 68, seed: 35214 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9459815546772063, Mean steps: 12.0\n",
      "Episode: 69, seed: 35241 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9467532467532462, Mean steps: 12.0\n",
      "Episode: 70, seed: 35412 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9475032010243273, Mean steps: 12.0\n",
      "Episode: 71, seed: 35421 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9482323232323228, Mean steps: 12.0\n",
      "Episode: 72, seed: 41235 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9489414694894143, Mean steps: 12.0\n",
      "Episode: 73, seed: 41253 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9484029484029479, Mean steps: 12.0\n",
      "Episode: 74, seed: 41325 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9478787878787873, Mean steps: 12.0\n",
      "Episode: 75, seed: 41352 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9485645933014348, Mean steps: 12.0\n",
      "Episode: 76, seed: 41523 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9492325855962215, Mean steps: 12.0\n",
      "Episode: 77, seed: 41532 Reward: -0.8181818181818183, Steps: 12 Mean reward: -0.9475524475524469, Mean steps: 12.0\n",
      "Episode: 78, seed: 42135 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9482163406214034, Mean steps: 12.0\n",
      "Episode: 79, seed: 42153 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9477272727272721, Mean steps: 12.0\n",
      "Episode: 80, seed: 42315 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9472502805836133, Mean steps: 12.0\n",
      "Episode: 81, seed: 42351 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9467849223946778, Mean steps: 12.0\n",
      "Episode: 82, seed: 42513 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.947426067907995, Mean steps: 12.0\n",
      "Episode: 83, seed: 42531 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9480519480519474, Mean steps: 12.0\n",
      "Episode: 84, seed: 43125 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9475935828876999, Mean steps: 12.0\n",
      "Episode: 85, seed: 43152 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9471458773784348, Mean steps: 12.0\n",
      "Episode: 86, seed: 43215 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9477533960292575, Mean steps: 12.0\n",
      "Episode: 87, seed: 43251 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9483471074380159, Mean steps: 12.0\n",
      "Episode: 88, seed: 43512 Reward: -0.8181818181818183, Steps: 12 Mean reward: -0.9468845760980585, Mean steps: 12.0\n",
      "Episode: 89, seed: 43521 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9464646464646458, Mean steps: 12.0\n",
      "Episode: 90, seed: 45123 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9470529470529463, Mean steps: 12.0\n",
      "Episode: 91, seed: 45132 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.947628458498023, Mean steps: 12.0\n",
      "Episode: 92, seed: 45213 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9472140762463336, Mean steps: 12.0\n",
      "Episode: 93, seed: 45231 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9468085106382971, Mean steps: 12.0\n",
      "Episode: 94, seed: 45312 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9473684210526309, Mean steps: 12.0\n",
      "Episode: 95, seed: 45321 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9469696969696962, Mean steps: 12.0\n",
      "Episode: 96, seed: 51234 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9465791940018736, Mean steps: 12.0\n",
      "Episode: 97, seed: 51243 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9461966604823739, Mean steps: 12.0\n",
      "Episode: 98, seed: 51324 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9458218549127632, Mean steps: 12.0\n",
      "Episode: 99, seed: 51342 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9454545454545447, Mean steps: 12.0\n",
      "Episode: 100, seed: 51423 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9450945094509442, Mean steps: 12.0\n",
      "Episode: 101, seed: 51432 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9447415329768262, Mean steps: 12.0\n",
      "Episode: 102, seed: 52134 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.944395410414827, Mean steps: 12.0\n",
      "Episode: 103, seed: 52143 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.944930069930069, Mean steps: 12.0\n",
      "Episode: 104, seed: 52314 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9445887445887436, Mean steps: 12.0\n",
      "Episode: 105, seed: 52341 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9451114922813028, Mean steps: 12.0\n",
      "Episode: 106, seed: 52413 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9447748513169065, Mean steps: 12.0\n",
      "Episode: 107, seed: 52431 Reward: -0.8181818181818183, Steps: 12 Mean reward: -0.9436026936026927, Mean steps: 12.0\n",
      "Episode: 108, seed: 53124 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9441201000834019, Mean steps: 12.0\n",
      "Episode: 109, seed: 53142 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9446280991735528, Mean steps: 12.0\n",
      "Episode: 110, seed: 53214 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9443079443079434, Mean steps: 12.0\n",
      "Episode: 111, seed: 53241 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9448051948051939, Mean steps: 12.0\n",
      "Episode: 112, seed: 53412 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9452936444086877, Mean steps: 12.0\n",
      "Episode: 113, seed: 53421 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9457735247208923, Mean steps: 12.0\n",
      "Episode: 114, seed: 54123 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9454545454545445, Mean steps: 12.0\n",
      "Episode: 115, seed: 54132 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9451410658307201, Mean steps: 12.0\n",
      "Episode: 116, seed: 54213 Reward: -0.9090909090909093, Steps: 12 Mean reward: -0.9448329448329439, Mean steps: 12.0\n",
      "Episode: 117, seed: 54231 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9453004622496138, Mean steps: 12.0\n",
      "Episode: 118, seed: 54312 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9457601222307095, Mean steps: 12.0\n",
      "Episode: 119, seed: 54321 Reward: -1.0000000000000002, Steps: 12 Mean reward: -0.9462121212121203, Mean steps: 12.0\n",
      "Total num episodes: 120 Success rate: 0.0, Mean reward: -0.9462121212121203, Mean steps: 12.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, -0.9462121212121203, 12.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.evaluate(n_episode = n_episode, eval_seeds = eval_seeds, render = False, videos_limit = videos_limit, videos_dir = videos_dir, run_name = run_name, run_type =run_type )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 8, seed: 13425 Reward: 1.0, Steps: 39 Mean reward: 0.31339031339031337, Mean steps: 39.77777777777778\n",
      "Episode: 9, seed: 13452 Reward: -0.02564102564102564, Steps: 40 Mean reward: 0.2794871794871795, Mean steps: 39.8\n",
      "Episode: 10, seed: 13524 Reward: 1.0, Steps: 39 Mean reward: 0.34498834498834496, Mean steps: 39.72727272727273\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m run_name \u001b[38;5;241m=\u001b[39m checkpoint_path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     65\u001b[0m run_type \u001b[38;5;241m=\u001b[39m checkpoint_path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m---> 67\u001b[0m sr, rw, sr \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_episode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_episode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_seeds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43meval_seeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideos_limit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvideos_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideos_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvideos_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_type\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m testing_res\u001b[38;5;241m.\u001b[39mappend((episode_timeout, sr, rw, sr))\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mres: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(episode_timeout,\u001b[38;5;250m \u001b[39msr,\u001b[38;5;250m \u001b[39mrw,\u001b[38;5;250m \u001b[39msr)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/Memory-RL-Codebase/eval/Passive_T_Maze_Flag/../../models/DTQN/dtqn/agents/dtqn.py:431\u001b[0m, in \u001b[0;36mDtqnAgent.evaluate\u001b[0;34m(self, n_episode, eval_seeds, render, videos_limit, videos_dir, run_name, run_type)\u001b[0m\n\u001b[1;32m    429\u001b[0m timestep \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    430\u001b[0m history \u001b[38;5;241m=\u001b[39m env_processing\u001b[38;5;241m.\u001b[39madd_to_history(history, obs)\n\u001b[0;32m--> 431\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\n\u001b[1;32m    433\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# print(f'Action :{action}, obs: {obs.shape}, reward: {reward}, terminated: {done}, info: {info}')\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/Memory-RL-Codebase/eval/Passive_T_Maze_Flag/../../models/DTQN/dtqn/agents/dtqn.py:297\u001b[0m, in \u001b[0;36mDtqnAgent.get_action\u001b[0;34m(self, history, epsilon)\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn)\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;66;03m# the policy network gets [1, timestep+1 x obs length] as input and\u001b[39;00m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;66;03m# outputs [1, timestep+1 x 4 outputs]\u001b[39;00m\n\u001b[1;32m    296\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_network(\n\u001b[0;32m--> 297\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs_tensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    300\u001b[0m     )\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;66;03m# We take the argmax of the last timestep's Q values\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;66;03m# In other words, select the highest q value action\u001b[39;00m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39margmax(q_values[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config_path = '/opt/Memory-RL-Codebase/configs/DTQN_configs/Passive_T_Maze_Flag/Dense/ICLR_exp_1/Passive_T_Maze_Flag_SHORT_TERM.yaml'\n",
    "testing_res = []\n",
    "\n",
    "for episode_timeout in [15, 20, 30]:\n",
    "\n",
    "    episode_timeout = 40\n",
    "    corridor_length = episode_timeout - 2\n",
    "    penalty = -1/(episode_timeout - 1)\n",
    "\n",
    "    device = torch.device('cuda:0')\n",
    "\n",
    "\n",
    "    with open(config_path, 'r') as file:\n",
    "        args = yaml.safe_load(file)\n",
    "\n",
    "    env = TMazeClassicPassive(episode_length=episode_timeout, \n",
    "                                corridor_length=corridor_length, \n",
    "                                goal_reward=1.0,\n",
    "                                penalty=penalty)\n",
    "\n",
    "    checkpoint_path = '/opt/Memory-RL-Codebase/autorun/checkpoints/Passive_T_Maze_Flag/DTQN/DTQN_Passive_T_Maze_Flag_SHORT_TERM_dense/2024_09_28-23_43_40.pt'\n",
    "\n",
    "\n",
    "    args['inembed'] = 64\n",
    "    args['context'] = episode_timeout\n",
    "\n",
    "\n",
    "    agent = get_agent(\n",
    "        args['model'],\n",
    "        env,\n",
    "        env,\n",
    "        args['obsembed'],\n",
    "        args['inembed'],\n",
    "        args['buf_size'],\n",
    "        device,\n",
    "        args['lr'],\n",
    "        args['batch'],\n",
    "        args['context'],\n",
    "        args['history'],\n",
    "        args['num_steps'],\n",
    "        # DTQN specific\n",
    "        args['heads'],\n",
    "        args['layers'],\n",
    "        args['dropout'],\n",
    "        args['identity'],\n",
    "        args['gate'],\n",
    "        args['pos'],\n",
    "    )\n",
    "    \n",
    "\n",
    "    ckp  = torch.load(checkpoint_path)\n",
    "    load_state_dict(agent.policy_network, ckp['policy_net_state_dict'])\n",
    "    load_state_dict(agent.target_network, ckp['target_net_state_dict'])\n",
    "\n",
    "    nums = [1, 2, 3, 4, 5]\n",
    "\n",
    "    eval_seeds = generate_permutations(nums)\n",
    "\n",
    "    videos_limit = len(eval_seeds) + 1\n",
    "    n_episode = len(eval_seeds)\n",
    "\n",
    "    videos_dir = '/opt/Memory-RL-Codebase/eval/Minigrid_Memory/DTQN'\n",
    "\n",
    "    run_name = checkpoint_path.split('/')[-1].strip('.pt')\n",
    "    run_type = checkpoint_path.split('/')[-2]\n",
    "\n",
    "    sr, rw, sr = agent.evaluate(n_episode = n_episode, eval_seeds = eval_seeds, render = False, videos_limit = videos_limit, videos_dir = videos_dir, run_name = run_name, run_type =run_type )\n",
    "\n",
    "    testing_res.append((episode_timeout, sr, rw, sr))\n",
    "\n",
    "    print(f'res: {(episode_timeout, sr, rw, sr)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
