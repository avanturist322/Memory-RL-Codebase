{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from environments.Passive_T_Maze_Flag.env.env_passive_t_maze_flag import TMazeClassicPassive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corridor_length = 88\n",
    "# episode_timeout = corridor_length + 2 # * DEFAULT CONFIGURATION: T = L + 2\n",
    "# desired_reward = 1 # * DEFAULT\n",
    "# penalty = -0.1\n",
    "\n",
    "episode_timeout = 15\n",
    "corridor_length = 13\n",
    "desired_reward = 1\n",
    "penalty = -0.0714"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TMazeClassicPassive(episode_length=episode_timeout, \n",
    "                          corridor_length=corridor_length, \n",
    "                          goal_reward=desired_reward,\n",
    "                          penalty=penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(4)\n",
      "Observation_space: MultiDiscrete([3 3 2 3], start=[-1 -1  0 -1])\n"
     ]
    }
   ],
   "source": [
    "print(f'Action space: {env.action_space}\\nObservation_space: {env.observation_space}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, -1,  1,  1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium\n",
    "gymnasium.spaces.MultiDiscrete(nvec = [3, 3, 2, 3], start=[-1, -1, 0, -1]).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.14605996,  0.78547776,  0.4864355 ], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gymnasium.spaces.Box(low=-1.0, high=1.0, shape=(3,), dtype=np.float32).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.contains(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  0,  1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.0000,  1.0000,  1.0000],\n",
       "        [-1.1867,  0.4191, -0.0621],\n",
       "        [ 0.4200,  0.8275, -0.4135]], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "# an Embedding module containing 10 tensors of size 3\n",
    "# embedding = nn.Embedding(10, 3)\n",
    "# # a batch of 2 samples of 4 indices each\n",
    "# input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 9]])\n",
    "# embedding(input)\n",
    "\n",
    "\n",
    "\n",
    "# example with padding_idx\n",
    "embedding = nn.Embedding(10, 3, padding_idx=0)\n",
    "input = torch.LongTensor([[0, 2, 0, 5]])\n",
    "embedding(input)\n",
    "\n",
    "# example of changing `pad` vector\n",
    "padding_idx = 0\n",
    "embedding = nn.Embedding(3, 3, padding_idx=padding_idx)\n",
    "embedding.weight\n",
    "with torch.no_grad():\n",
    "    embedding.weight[padding_idx] = torch.ones(3)\n",
    "embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  1, -1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_obs = env.observation_space.sample()\n",
    "sample_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 2, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.nvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1,  0, -1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = nn.Embedding(max(env.observation_space.nvec) + 1, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1828e+00, -1.6288e+00,  6.5457e-01, -7.5468e-01,  4.3747e-04,\n",
       "          5.7303e-01,  1.8187e+00, -3.8963e-01],\n",
       "        [ 1.1828e+00, -1.6288e+00,  6.5457e-01, -7.5468e-01,  4.3747e-04,\n",
       "          5.7303e-01,  1.8187e+00, -3.8963e-01],\n",
       "        [-3.7157e-01, -3.6561e-01,  4.4892e-01,  1.7652e-01,  7.8725e-01,\n",
       "          3.6055e-01, -7.4463e-01, -6.9592e-02],\n",
       "        [-3.6550e-01,  1.4621e+00,  6.3402e-01, -4.1693e-02,  1.4245e+00,\n",
       "         -1.5372e-02, -9.3236e-01, -7.6748e-01]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb(torch.IntTensor(sample_obs - env.observation_space.start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sizes = 4\n",
    "embed_per_obs_dim = 8\n",
    "outer_embed_size = 64\n",
    "obs_dim = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Sequential(\n",
    "    nn.Embedding(vocab_sizes, embed_per_obs_dim),\n",
    "    nn.Flatten(start_dim=-2),\n",
    "    nn.Linear(embed_per_obs_dim * obs_dim, outer_embed_size),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.start if 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0994, -0.1877,  0.2686,  0.8510,  0.1187,  0.2928, -0.2609,  0.7188,\n",
       "         0.2015, -0.3910, -0.2819, -0.2357, -1.0178, -0.4905,  0.5011, -0.4083,\n",
       "        -0.2027, -0.8734, -0.1038, -0.1175, -0.7441,  0.4236,  0.4068, -1.0264,\n",
       "        -0.5201,  0.1866, -0.2006,  0.2609, -0.1636, -0.2310, -0.0293, -0.0423,\n",
       "        -0.7181, -0.5914,  0.0048,  0.1936,  0.5824, -0.1123,  0.2089,  0.1284,\n",
       "         0.0791, -0.4059, -0.6135, -0.4111, -0.0912,  0.1447, -0.2116, -0.9837,\n",
       "        -0.4801,  0.0952,  0.6121,  0.6372, -0.4187, -0.1972,  0.3429, -0.1803,\n",
       "        -0.6344, -0.4401,  0.2661, -0.8525,  0.5618, -0.9396, -0.3906,  0.6107],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(torch.IntTensor(np.array([1, -1, 0, 1]) - env.observation_space.start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0],\n",
       "        [0, 1, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.IntTensor([[1, 4, 4], [1, 4, 4]]) - torch.IntTensor([1, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 4], dtype=torch.int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.IntTensor([1, 3, 4]).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action :0, obs: [ 0.  0. -1.], reward: -0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 1.], reward: -0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0.], reward: -0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  0. -1.], reward: -0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0.], reward: -0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 1.], reward: -0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  0. -1.], reward: -0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0.], reward: -0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  0. -1.], reward: -0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 1.], reward: -0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  0. -1.], reward: -0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  0. -1.], reward: -0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  1. -1.], reward: -0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  1. -1.], reward: -0.0714, terminated: False, info: {}\n",
      "Action :0, obs: [0. 1. 0.], reward: 0.0, terminated: True, info: {'reward': -0.0714, 'length': 15}\n"
     ]
    }
   ],
   "source": [
    "# new \n",
    "\n",
    "done = False\n",
    "obs = env.reset()  # * {y, hint, flag, noise}\n",
    "\n",
    "\n",
    "history = [obs]\n",
    "while not done:\n",
    "    action = 0 #env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    print(f'Action :{action}, obs: {obs}, reward: {reward}, terminated: {done}, info: {info}')\n",
    "    history.append(obs)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corridor_length = 88\n",
    "episode_timeout = corridor_length + 2 # * DEFAULT CONFIGURATION: T = L + 2\n",
    "desired_reward = 1 # * DEFAULT\n",
    "penalty = -0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init obs:[ 0. -1.  0. -1.]\n",
      "Action :0, obs: [0. 0. 0. 1.], reward: -0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  0.  0. -1.], reward: -0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 0.], reward: -0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 0.], reward: -0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 0.], reward: -0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 1.], reward: -0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 0.], reward: -0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  0.  0. -1.], reward: -0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  0.  0. -1.], reward: -0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 1.], reward: -0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 1.], reward: -0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  0.  0. -1.], reward: -0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  0.  1. -1.], reward: -0.0, terminated: False, info: {}\n",
      "Action :3, obs: [-1.  0.  0.  1.], reward: 1, terminated: True, info: {'reward': 1.0, 'length': 14}\n"
     ]
    }
   ],
   "source": [
    "# OPTIMAL POLICY\n",
    "\n",
    "# env = TMazeClassicPassive(episode_length=episode_timeout, \n",
    "#                           corridor_length=corridor_length, \n",
    "#                           goal_reward=desired_reward)\n",
    "\n",
    "obs = env.reset(seed=41)  # * {y, hint, flag, noise}\n",
    "x0 = env.x\n",
    "cue = obs[1]\n",
    "done = False\n",
    "step = 0\n",
    "R = 0\n",
    "print(f'Init obs:{obs}')\n",
    "# obss = []\n",
    "while not done:\n",
    "    # obss.append(env.render())\n",
    "    a = 0 if step != corridor_length else (1 if cue == 1 else 3)\n",
    "    obs, r, done, info = env.step(a)\n",
    "    step += 1\n",
    "    R += r\n",
    "    x = env.x\n",
    "\n",
    "    print(f'Action :{a}, obs: {obs}, reward: {r}, terminated: {done}, info: {info}')\n",
    "\n",
    "\n",
    "# obss.append(env.render())\n",
    "env.close()\n",
    "\n",
    "# print(step,'\\t', obs, '\\t\\t', R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init obs:[ 0. -1.  0. -1.]\n",
      "Action :0, obs: [0. 0. 0. 1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  0.  0. -1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 0.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 0.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 0.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 0.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  0.  0. -1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  0.  0. -1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  0.  0. -1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  0.  0. -1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 0.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 0.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  0.  0. -1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  0.  0. -1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  0.  0. -1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  0.  0. -1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 0.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 0.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 0.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  0.  0. -1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 0.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 0.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 0.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  0.  0. -1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 0.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  0.  0. -1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  0.  0. -1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  0.  0. -1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 0.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  0.  0. -1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 0.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 0.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  0.  0. -1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 0.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  0.  0. -1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 0.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 0.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  0.  0. -1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  0.  0. -1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 0.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  0.  0. -1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 0.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 0.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 0.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 0.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  0.  0. -1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  0.  0. -1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 0.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  0.  0. -1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  0.  0. -1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 0.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 0.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 0.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 0.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 0.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  0.  0. -1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 0.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 0.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 0. 1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [ 0.  0.  0. -1.], reward: 0.0, terminated: False, info: {}\n",
      "Action :0, obs: [0. 0. 1. 0.], reward: 0.0, terminated: False, info: {}\n",
      "Action :3, obs: [-1.  0.  0.  1.], reward: 1, terminated: True, info: {'reward': 1.0, 'length': 89}\n"
     ]
    }
   ],
   "source": [
    "# OPTIMAL POLICY\n",
    "\n",
    "env = TMazeClassicPassive(episode_length=episode_timeout, \n",
    "                          corridor_length=corridor_length, \n",
    "                          goal_reward=desired_reward)\n",
    "\n",
    "obs = env.reset(seed=41)  # * {y, hint, flag, noise}\n",
    "x0 = env.x\n",
    "cue = obs[1]\n",
    "done = False\n",
    "step = 0\n",
    "R = 0\n",
    "print(f'Init obs:{obs}')\n",
    "# obss = []\n",
    "while not done:\n",
    "    # obss.append(env.render())\n",
    "    a = 0 if step != corridor_length else (1 if cue == 1 else 3)\n",
    "    obs, r, done, info = env.step(a)\n",
    "    step += 1\n",
    "    R += r\n",
    "    x = env.x\n",
    "\n",
    "    print(f'Action :{a}, obs: {obs}, reward: {r}, terminated: {done}, info: {info}')\n",
    "\n",
    "\n",
    "# obss.append(env.render())\n",
    "env.close()\n",
    "\n",
    "# print(step,'\\t', obs, '\\t\\t', R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from moviepy.editor import ImageSequenceClip\n",
    "# import numpy as np\n",
    "\n",
    "# desired_resolution = (945, 540)\n",
    "# original_aspect_ratio = 112 / 64\n",
    "# width = int(desired_resolution[0] * original_aspect_ratio)\n",
    "# height = desired_resolution[1]\n",
    "\n",
    "# observations = [np.squeeze(o) for o in obss]\n",
    "\n",
    "# # Create ImageSequenceClip\n",
    "# clip = ImageSequenceClip(observations, fps=24)\n",
    "# clip = clip.resize(width=width, height=height)\n",
    "\n",
    "# # Display the modified clip\n",
    "# clip.ipython_display(maxduration=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "memory_rl_tmaze_flag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
